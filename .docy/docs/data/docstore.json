[["0",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Parse Arguments}\n    B --> C[Get List of Remote Branches]\n    C --> D[Clone Repo to temp_repo]\n    D --> E{For Each Branch}\n    E -->|For Each| F[Switch to Branch]\n    F --> G{Check .github/workflows Exists}\n    G -->|Exists| H[Delete .github/workflows Folder]\n    H --> I[Add Changes]\n    I --> J[Commit Changes]\n    J --> K[Push Changes]\n    G -->|Does Not Exist| L[Print Folder Not Found]\n    K --> M{More Branches?}\n    L --> M\n    M -->|Yes| F\n    M -->|No| N[Cleanup: Delete temp_repo]\n    N --> O[End]\n```","metadata":{"source":".docy/docs/markdown/collect/cleanup/delete_gh_workflows.md"}}],["1",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Parse Arguments}\n    B --> C[Get Conda Environments]\n    C --> D{Check Each Environment}\n    D -->|For Each| E[Remove Environment]\n    E --> F[Delete Folders with Prefix]\n    F --> G[End]\n\n    classDef startend fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef operation fill:#bbf,stroke:#f66,stroke-width:2px;\n    classDef condition fill:#fe9,stroke:#333,stroke-width:2px;\n    class A startend;\n    class B,C,F,G operation;\n    class D,E condition;\n```","metadata":{"source":".docy/docs/markdown/collect/cleanup/remove_envs.md"}}],["2",{"pageContent":"```mermaid\ngraph TD\n    subgraph delete_gh_workflows\n        A[Start] --> B{Parse Arguments}\n        B --> C[Get List of Remote Branches]\n        C --> D[Clone Repo to temp_repo]\n        D --> E{For Each Branch}\n        E -->|For Each| F[Switch to Branch]\n        F --> G{Check .github/workflows Exists}\n        G -->|Exists| H[Delete .github/workflows Folder]\n        H --> I[Add Changes]\n        I --> J[Commit Changes]\n        J --> K[Push Changes]\n        G -->|Does Not Exist| L[Print Folder Not Found]\n        K --> M{More Branches?}\n        L --> M\n        M -->|Yes| F\n        M -->|No| N[Cleanup: Delete temp_repo]\n        N --> O[End]\n    end\n\n    subgraph remove_envs\n        P[Start] --> Q{Parse Arguments}\n        Q --> R[Get Conda Environments]\n        R --> S{Check Each Environment}\n        S -->|For Each| T[Remove Environment]\n        T --> U[Delete Folders with Prefix]\n        U --> V[End]\n    end\n\n    style delete_gh_workflows fill:#f9f,stroke:#333,stroke-width:2px\n    style remove_envs fill:#bbf,stroke:#f66,stroke-width:2px\n\n    %% Connections between subgraphs to illustrate potential workflow or dependencies\n    %% Assuming these scripts might be part of a larger cleanup or setup process\n    V --> A : \"Subsequent Cleanup\"\n    O --> P : \"Preceding Task\"\n```","metadata":{"source":".docy/docs/markdown/collect/cleanup/summary.md"}}],["3",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Load Environment Variables}\n    B --> C[Parse Command Line Arguments]\n    C --> D[Split Repositories Among Tokens]\n    D --> E[Construct Data Files for Each Token]\n    E --> F{Check if PR Data Exists}\n    F -->|Yes| G[Skip PR Data Retrieval]\n    F -->|No| H[Retrieve PR Data]\n    H --> I{Check if Task Data Exists}\n    I -->|Yes| J[Skip Task Data Creation]\n    I -->|No| K[Create Task Data]\n    K --> L[End]\n    G --> L\n    J --> L\n```","metadata":{"source":".docy/docs/markdown/collect/get_tasks_pipeline.md"}}],["4",{"pageContent":"```mermaid\nflowchart TD\n    A[Start selenium driver] --> B[Open top 5000 pypi page]\n    B --> C[Click to show top 5000 packages]\n    C --> D[Retrieve HTML for packages]\n    D --> E[Extract package details from HTML]\n    E --> F{Check if package URL exists in file}\n    F -->|Yes| G[Continue to next package]\n    F -->|No| H[Get package GitHub URL]\n    H --> I{GitHub URL found?}\n    I -->|Yes| J[Fetch stars and pulls from GitHub API]\n    I -->|No| K[Skip GitHub data]\n    J --> L[Write package data to file]\n    K --> L\n    G --> M[End of package list?]\n    L --> M\n    M -->|Yes| N[End]\n    M -->|No| E\n```\nThis flowchart represents the process of extracting package statistics from a top 5000 PyPI packages page, enriching the data with GitHub statistics, and writing the results to a file. The process starts with initializing a Selenium driver to navigate the web page, clicking to reveal the top 5000 packages, and then parsing the HTML to extract package details. For each package, it checks if the URL already exists in the output file to avoid duplicates. If not, it attempts to find the package's GitHub URL from the package's PyPI page. If a GitHub URL is found, it fetches star and pull request counts using the GitHub API. Finally, it writes the package data, including GitHub statistics if available, to a file, iterating through all packages in the list.","metadata":{"source":".docy/docs/markdown/collect/get_top_pypi.md"}}],["5",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{For each repo in repos}\n    B -->|For each repo| C[Make mirror repo for repo]\n    C --> D{Check if make_repo was successful}\n    D -->|Success| E[Print success message]\n    D -->|Error| F[Print error message]\n    E --> G[End]\n    F --> G\n```","metadata":{"source":".docy/docs/markdown/collect/make_repo/call_make_repo.md"}}],["6",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Check if target repository exists}\n    B -->|No| C[Exit]\n    B -->|Yes| D[Set organization and repository names]\n    D --> E{Check if new repository already exists}\n    E -->|Yes| F[Inform repository exists, then exit]\n    E -->|No| G[Create mirror repository]\n    G --> H{Check if repository creation was successful}\n    H -->|No| I[Inform failure, then exit]\n    H -->|Yes| J[Inform repository created successfully]\n    J --> K{Check if local repository directory exists}\n    K -->|Yes| L[Inform directory exists, then exit]\n    K -->|No| M[Clone the target repository]\n    M --> N[Perform mirror push of files]\n    N --> O[Remove the target repository directory]\n    O --> P[Clone the mirror repository]\n    P --> Q{Check if .github/workflows exists}\n    Q -->|Yes| R[Remove .github/workflows directory]\n    R --> S[Commit and push changes]\n    Q -->|No| T[Inform .github/workflows does not exist]\n    S --> U[Cleanup cloned mirror repository]\n    T --> U\n    U --> V[End]\n```","metadata":{"source":".docy/docs/markdown/collect/make_repo/make_repo.md"}}],["7",{"pageContent":"```mermaid\ngraph TD\n    A[Start call_make_repo.py] --> B{For each repo in repos}\n    B -->|For each repo| C[Make mirror repo for repo using make_repo.sh]\n    C --> D{Check if target repository exists}\n    D -->|No| E[Exit make_repo.sh]\n    D -->|Yes| F[Set organization and repository names]\n    F --> G{Check if new repository already exists}\n    G -->|Yes| H[Inform repository exists, then exit make_repo.sh]\n    G -->|No| I[Create mirror repository]\n    I --> J{Check if repository creation was successful}\n    J -->|No| K[Inform failure, then exit make_repo.sh]\n    J -->|Yes| L[Inform repository created successfully]\n    L --> M{Check if local repository directory exists}\n    M -->|Yes| N[Inform directory exists, then exit make_repo.sh]\n    M -->|No| O[Clone the target repository]\n    O --> P[Perform mirror push of files]\n    P --> Q[Remove the target repository directory]\n    Q --> R[Clone the mirror repository]\n    R --> S{Check if .github/workflows exists}\n    S -->|Yes| T[Remove .github/workflows directory]\n    T --> U[Commit and push changes]\n    S -->|No| V[Inform .github/workflows does not exist]\n    U --> W[Cleanup cloned mirror repository]\n    V --> W\n    W --> X[End make_repo.sh]\n    X --> Y{Check if make_repo was successful in call_make_repo.py}\n    Y -->|Success| Z[Print success message]\n    Y -->|Error| AA[Print error message]\n    Z --> AB[End call_make_repo.py]\n    AA --> AB\n```","metadata":{"source":".docy/docs/markdown/collect/make_repo/summary.md"}}],["8",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Is GitHub token provided?}\n    B -- Yes --> C[Use provided token]\n    B -- No --> D[Use GITHUB_TOKEN from environment]\n    C --> E[Create Repo object]\n    D --> E\n    E --> F[Iterate over all pull requests in the repository]\n    F --> G[Extract resolved issues for each pull]\n    G --> H[Convert pull request object to dictionary]\n    H --> I[Log pull request data to output file]\n    I --> J[End]\n```","metadata":{"source":".docy/docs/markdown/collect/print_pulls.md"}}],["9",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Check for .env file}\n    B -->|Exists| C[Use GITHUB_TOKENS from .env]\n    B -->|Does not exist| D[Proceed without GITHUB_TOKENS]\n    C --> E[Set up parallelization with tokens]\n    D --> E\n    E --> F[Run get_tasks_pipeline.py script]\n    F --> G[Specify repositories to analyze]\n    G -->|scikit-learn/scikit-learn| H[Fetch PRs for scikit-learn]\n    G -->|pallets/flask| I[Fetch PRs for Flask]\n    H --> J[Save PRs to specified path]\n    I --> J\n    J --> K[Save tasks to specified path]\n    K --> L[End]\n```","metadata":{"source":".docy/docs/markdown/collect/run_get_tasks_pipeline.md"}}],["10",{"pageContent":"```mermaid\ngraph TD\n    subgraph swe_bench_collect\n        A[Start] --> B[get_top_pypi.py: Fetch PyPI Packages]\n        B --> C[get_tasks_pipeline.py: Process Repositories]\n        C --> D[print_pulls.py: Log Pull Requests]\n        D --> E[utils.py: Utility Functions]\n        E --> F[End]\n    end\n\n    subgraph cleanup\n        G[cleanup: Start] --> H[delete_gh_workflows: Delete GitHub Workflows]\n        H --> I[remove_envs: Remove Conda Environments]\n        I --> J[cleanup: End]\n    end\n\n    subgraph make_repo\n        K[make_repo: Start] --> L[call_make_repo.py: Initiate Repository Creation]\n        L --> M[make_repo.sh: Create and Configure Repositories]\n        M --> N[make_repo: End]\n    end\n\n    %% Connections between main tasks and cleanup/make_repo subgraphs\n    F --> G : \"Pre-cleanup\"\n    J --> K : \"Post-cleanup, Pre-make_repo\"\n    N --> A : \"Cycle back to start\"\n\n    style swe_bench_collect fill:#f9f,stroke:#333,stroke-width:4px\n    style cleanup fill:#bbf,stroke:#f66,stroke-width:4px\n    style make_repo fill:#bff,stroke:#333,stroke-width:4px\n```","metadata":{"source":".docy/docs/markdown/collect/summary.md"}}],["11",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Repo Initialization}\n    B --> C[API Call Wrapper with Rate Limit Handling]\n    C -->|API Call Success| D[Extract Resolved Issues]\n    C -->|Rate Limit Exceeded| E[Wait and Retry]\n    C -->|Resource Not Found| F[Return None]\n    D --> G[Get All Loop for Paginated API Endpoints]\n    G --> H[Get All Issues]\n    G --> I[Get All Pulls]\n    H --> J[Extract Problem Statement and Hints]\n    I --> K[Extract Patches]\n    J --> L[Extract Hints from Comments]\n    K --> M[Convert Diff to Patch Format]\n    L --> N[End]\n    M --> N\n```\nThis flowchart represents the sequence of operations performed by the code in the `swe-bench` project. It starts with the initialization of a `Repo` object, which involves making an API call. If the API call is successful, the process moves to extracting resolved issues from pull requests. If the rate limit is exceeded during any API call, the system waits and retries. If a resource is not found, it returns `None`.\n\nThe `get_all_loop` method is used for retrieving all items from paginated API endpoints, which is a crucial step for both getting all issues and all pull requests from the repository. These issues and pull requests are then used to extract problem statements, hints, and patches. Hints are extracted from comments associated with a pull request before the first commit, and patches are generated by converting the diff obtained from a pull request into a patch format, excluding specific metadata and categorizing changes into either test or general changes. The process ends after extracting hints and patches.","metadata":{"source":".docy/docs/markdown/collect/utils.md"}}],["12",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Check Python Version}\n    B -->|3.9| C[Setup Environment]\n    C --> D[Install Dependencies]\n    D --> E[Install pip packages]\n    E --> F[beautifulsoup4 for HTML parsing]\n    E --> G[chardet for character encoding detection]\n    E --> H[ghapi for GitHub API interaction]\n    E --> I[GitPython for Git operations]\n    E --> J[python-dotenv for .env file management]\n    E --> K[requests for HTTP requests]\n    E --> L[rich for rich text and beautiful formatting in the terminal]\n    E --> M[transformers for state-of-the-art machine learning]\n    D --> N[Install conda-forge packages]\n    N --> O[gh for GitHub CLI]\n    O --> P[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style P fill:#f9f,stroke:#333,stroke-width:4px\n```","metadata":{"source":".docy/docs/markdown/environment.md"}}],["13",{"pageContent":"```mermaid\nflowchart TB\n    subgraph swe_bench [\"swe-bench\"]\n    direction TB\n    create_maps[(\"Create Version-to-Install Maps\")]\n    update_maps[(\"Update Version-to-Install Maps with Additional Versions\")]\n    consolidate_maps[(\"Consolidate Maps into MAP_VERSION_TO_INSTALL\")]\n    define_constants[(\"Define Constants for Test Frameworks, Requirement Paths, and Environment Paths\")]\n    define_keys[(\"Define Evaluation Keys\")]\n    define_logging[(\"Define Logging Constants\")]\n    define_misc[(\"Define Miscellaneous Constants\")]\n\n    create_maps --> update_maps\n    update_maps --> consolidate_maps\n    consolidate_maps --> define_constants\n    define_constants --> define_keys\n    define_keys --> define_logging\n    define_logging --> define_misc\n    end\n\n    subgraph larger_project [\"Larger Project\"]\n    direction TB\n    init_env[(\"Initialize Environment Based on MAP_VERSION_TO_INSTALL\")]\n    apply_patches[(\"Apply Patches if Necessary\")]\n    install_dependencies[(\"Install Dependencies\")]\n    run_tests[(\"Run Tests Using Defined Test Frameworks\")]\n    evaluate[(\"Evaluate Test Outcomes\")]\n    log_results[(\"Log Results\")]\n\n    init_env --> apply_patches\n    apply_patches --> install_dependencies\n    install_dependencies --> run_tests\n    run_tests --> evaluate\n    evaluate --> log_results\n    end\n\n    swe_bench -->|Used for| larger_project\n```\nThis flowchart represents the low-level purpose of the code within the `swe-bench` project. It starts with the creation and updating of version-to-install maps for various libraries and frameworks. These maps are then consolidated into a single map (`MAP_VERSION_TO_INSTALL`) which is used throughout the project. Constants for test frameworks, requirement paths, and environment paths are defined, along with evaluation keys and logging constants. Miscellaneous constants are also defined to support the project's operations.\n\nIn the context of the larger project, these maps and constants are utilized to initialize environments specific to different versions of libraries/frameworks, apply necessary patches, install dependencies, run tests using defined frameworks, evaluate test outcomes, and log the results. This process ensures that the `swe-bench` project can dynamically support a wide range of library versions and configurations for testing and evaluation purposes.","metadata":{"source":".docy/docs/markdown/harness/constants.md"}}],["14",{"pageContent":"```mermaid\nflowchart TB\n    subgraph swe_bench\n    direction TB\n    start([Start]) --> init_logging([Initialize Logging])\n    init_logging --> exec_wrapper([ExecWrapper Class])\n    exec_wrapper --> testbed_context_manager([TestbedContextManager Class])\n    testbed_context_manager --> setup_testbed([Set Up Testbed])\n    setup_testbed --> create_conda_env([Create Conda Environments])\n    create_conda_env --> clone_repo([Clone Repositories])\n    clone_repo --> install_dependencies([Install Dependencies])\n    install_dependencies --> task_env_context_manager([TaskEnvContextManager Class])\n    task_env_context_manager --> enter_task_env([Enter Task Environment])\n    enter_task_env --> reset_task_env([Reset Task Environment])\n    reset_task_env --> run_install_task([Run Installation Task])\n    run_install_task --> apply_patch([Apply Patch])\n    apply_patch --> run_tests_task([Run Tests Task])\n    run_tests_task --> exit_task_env([Exit Task Environment])\n    exit_task_env --> end([End])\n    end\n    end\n```\nThis flowchart represents the sequence of operations performed by the code in the `swe-bench` project. It starts with initializing logging, followed by the execution of the `ExecWrapper` class for running subprocess commands with specific arguments. The `TestbedContextManager` class sets up the testbed, including creating Conda environments, cloning repositories, and installing dependencies. The `TaskEnvContextManager` class manages the execution context for individual task instances, including entering and exiting the task environment, resetting the environment, running installation tasks, applying patches, and running test tasks.","metadata":{"source":".docy/docs/markdown/harness/context_manager.md"}}],["15",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Check Arguments}\n    B --> C[Get Predictions]\n    C --> D{Skip Existing?}\n    D -->|Yes| E[Filter Predictions]\n    D -->|No| F[Split Predictions]\n    E --> F\n    F --> G{Check Num Workers}\n    G -->|One| H[Setup Testbed Single]\n    G -->|Multiple| I[Setup Testbed Parallel]\n    H --> J[End]\n    I --> J\n    \n    subgraph overwrite_ablation\n        OA[Start Overwrite Ablation] --> OB{Check Full Output}\n        OB -->|Missing| OC[Log & Skip]\n        OB -->|Present| OD{Reset Task Env}\n        OD -->|Fail| OE[Log & Skip]\n        OD -->|Success| OF{Run Install Task}\n        OF -->|Fail| OE\n        OF -->|Success| OG{Apply Test Patch}\n        OG -->|Fail| OE\n        OG -->|Success| OH[Overwrite Files]\n        OH --> OI{Run Tests Task}\n        OI -->|Fail| OE\n        OI -->|Success| OJ[End Overwrite Ablation]\n    end\n    \n    subgraph evaluate_predictions\n        EP[Start Evaluate Predictions] --> EQ{Setup Task Env Context}\n        EQ --> ER{Reset Task Env}\n        ER -->|Fail| ES[Continue]\n        ER -->|Success| ET{Apply Prediction Patch}\n        ET -->|Fail| EU[Extract & Apply Minimal Patch]\n        EU -->|Fail| ES\n        EU -->|Success| EV[Revert Patch]\n        ET -->|Success| EV\n        EV --> EW{Run Install & Test}\n        EW -->|Fail| ES\n        EW -->|Success| EX[End Evaluate Predictions]\n    end\n    \n    style overwrite_ablation fill:#f9f,stroke:#333,stroke-width:2px\n    style evaluate_predictions fill:#bbf,stroke:#333,stroke-width:2px\n```\n\nThis mermaid diagram illustrates the flow of the code in the `swe-bench` project, focusing on the low-level operations and how they might be utilized in a larger context. The diagram is divided into main operations, including handling command-line arguments, processing predictions, and the detailed steps within the `overwrite_ablation` and `evaluate_predictions` functions, highlighting their roles in the project's workflow.","metadata":{"source":".docy/docs/markdown/harness/engine_evaluation.md"}}],["16",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Validate Command Line Arguments}\n    B --> C[Load Task Instances]\n    C --> D[Split Task Instances]\n    D --> E{Check Number of Workers}\n    E -->|One Worker| F[Setup Testbed for Single Worker]\n    E -->|Multiple Workers| G[Setup Testbed for Multiple Workers]\n    F --> H[Verify Task Instances Sequentially]\n    G --> I[Verify Task Instances in Parallel]\n    H --> J[End]\n    I --> J\n    F --> K[Create Testbed Context]\n    G --> L[Create Multiple Testbed Contexts]\n    K --> M[Run Tasks Sequentially in Context]\n    L --> N[Run Tasks in Parallel across Contexts]\n    M --> O[Task Environment Setup]\n    N --> O\n    O --> P{Check if Task Instance Should be Skipped}\n    P -->|Yes| Q[Skip Task Instance]\n    P -->|No| R[Reset Task Environment]\n    R --> S[Run Install Task]\n    S --> T[Apply Test Patch]\n    T --> U[Run Tests Task (Test Patch)]\n    U --> V[Apply Gold Patch]\n    V --> W[Run Tests Task (Gold Patch)]\n    Q --> X[Continue to Next Task Instance]\n    W --> X\n    X -->|All Task Instances Processed| J\n```\nThis mermaid diagram illustrates the flow of operations within the provided code, focusing on the validation of command line arguments, the handling of task instances (including their distribution among workers), and the detailed steps taken to verify each task instance within its environment.","metadata":{"source":".docy/docs/markdown/harness/engine_validation.md"}}],["17",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Validate Arguments}\n    B -->|Invalid| C[Raise ValueError]\n    B -->|Valid| D[Load Tasks]\n    D --> E{Tasks Source}\n    E -->|Local Path| F[Load from Path]\n    E -->|dev/test| G[Load from HuggingFace Datasets]\n    F --> H[Verify Tasks Format]\n    G --> H\n    H --> I[Validate Predictions]\n    I --> J[Group Predictions by Model]\n    J --> K[For Each Model]\n    K --> L[Group Predictions by Repo and Version]\n    L --> M[For Each Repo/Version]\n    M --> N[Create Testbed Folder]\n    N --> O[Save Predictions to File]\n    O --> P{Skip Existing Checks}\n    P -->|Yes| Q[Filter Already Evaluated Predictions]\n    P -->|No| R[Proceed Without Filtering]\n    Q --> S[Log Info & Skip if All Exist]\n    R --> S\n    S --> T{Any Predictions to Evaluate?}\n    T -->|No| U[Log Info & Exit]\n    T -->|Yes| V[Prepare Evaluation Arguments]\n    V --> W{Run Evaluation}\n    W -->|Single Process| X[Sequential Evaluation]\n    W -->|Multiple Processes| Y[Parallel Evaluation]\n    X --> Z[Clean Up]\n    Y --> Z\n    Z --> AA[End]\n```","metadata":{"source":".docy/docs/markdown/harness/run_evaluation.md"}}],["18",{"pageContent":"```mermaid\ngraph TD\n    A[Start engine_validation.py] --> B{Check if instances_path, log_dir, and temp_dir are provided}\n    B -->|Yes| C[Initialize with provided paths]\n    B -->|No| Z[Exit with error: Paths not provided]\n    C --> D{Check if num_workers is provided}\n    D -->|Yes| E[Set number of workers]\n    D -->|No| F[Use default number of workers]\n    E --> G[Start validation process]\n    F --> G\n    G --> H{Is verbose mode on?}\n    H -->|Yes| I[Run in verbose mode: Show detailed logs]\n    H -->|No| J[Run in silent mode: Show minimal logs]\n    I --> K[Perform validation on task instances]\n    J --> K\n    K --> L{Validation Complete}\n    L --> M[Generate and save logs to log_dir]\n    L --> N[Temporary files saved to temp_dir]\n    M --> O[End of engine_validation.py]\n    N --> O\n```\nThis flowchart represents the steps executed by the `engine_validation.py` script within the context of its operational environment, focusing on the initialization, configuration, and execution phases, including error handling and output management.","metadata":{"source":".docy/docs/markdown/harness/run_validation.md"}}],["19",{"pageContent":"```mermaid\nflowchart TB\n    subgraph constants_section[\"constants.py\"]\n    create_maps((\"Create Version-to-Install Maps\")) --> update_maps((\"Update Version-to-Install Maps\"))\n    update_maps --> consolidate_maps((\"Consolidate Maps\"))\n    consolidate_maps --> define_constants((\"Define Constants\"))\n    define_constants --> define_keys((\"Define Evaluation Keys\"))\n    define_keys --> define_logging((\"Define Logging Constants\"))\n    define_logging --> define_misc((\"Define Miscellaneous Constants\"))\n    end\n\n    subgraph context_manager_section[\"context_manager.py\"]\n    start((\"Start\")) --> init_logging((\"Initialize Logging\"))\n    init_logging --> exec_wrapper((\"ExecWrapper Class\"))\n    exec_wrapper --> testbed_context_manager((\"TestbedContextManager Class\"))\n    testbed_context_manager --> setup_testbed((\"Set Up Testbed\"))\n    setup_testbed --> create_conda_env((\"Create Conda Environments\"))\n    create_conda_env --> clone_repo((\"Clone Repositories\"))\n    clone_repo --> install_dependencies_cm((\"Install Dependencies\"))\n    install_dependencies_cm --> task_env_context_manager((\"TaskEnvContextManager Class\"))\n    task_env_context_manager --> enter_task_env((\"Enter Task Environment\"))\n    enter_task_env --> reset_task_env((\"Reset Task Environment\"))\n    reset_task_env --> run_install_task_cm((\"Run Installation Task\"))\n    run_install_task_cm --> apply_patch_cm((\"Apply Patch\"))\n    apply_patch_cm --> run_tests_task_cm((\"Run Tests Task\"))\n    run_tests_task_cm --> exit_task_env((\"Exit Task Environment\"))\n    exit_task_env --> end_cm((\"End\"))\n    end\n\n    subgraph engine_evaluation_section[\"engine_evaluation.py\"]\n    check_arguments((\"Check Arguments\")) --> get_predictions((\"Get Predictions\"))\n    get_predictions --> skip_existing((\"Skip Existing?\"))\n    skip_existing -->|Yes| filter_predictions((\"Filter Predictions\"))\n    skip_existing -->|No| split_predictions((\"Split Predictions\"))\n    filter_predictions --> split_predictions\n    split_predictions --> check_num_workers((\"Check Num Workers\"))\n    check_num_workers -->|One| setup_testbed_single((\"Setup Testbed Single\"))\n    check_num_workers -->|Multiple| setup_testbed_parallel((\"Setup Testbed Parallel\"))\n    setup_testbed_single --> overwrite_ablation((\"Overwrite Ablation\"))\n    setup_testbed_parallel --> evaluate_predictions((\"Evaluate Predictions\"))\n    end\n\n    subgraph engine_validation_section[\"engine_validation.py\"]\n    validate_cli_args((\"Validate CLI Arguments\")) --> load_task_instances((\"Load Task Instances\"))\n    load_task_instances --> split_task_instances((\"Split Task Instances\"))\n    split_task_instances --> check_num_workers_ev((\"Check Number of Workers\"))\n    check_num_workers_ev -->|One Worker| setup_testbed_single_ev((\"Setup Testbed for Single Worker\"))\n    check_num_workers_ev -->|Multiple Workers| setup_testbed_parallel_ev((\"Setup Testbed for Multiple Workers\"))\n    setup_testbed_single_ev --> verify_task_instances_seq((\"Verify Task Instances Sequentially\"))\n    setup_testbed_parallel_ev --> verify_task_instances_par((\"Verify Task Instances in Parallel\"))\n    verify_task_instances_seq --> create_testbed_context((\"Create Testbed Context\"))\n    verify_task_instances_par --> create_multiple_testbed_contexts((\"Create Multiple Testbed Contexts\"))\n    create_testbed_context --> run_tasks_seq_in_context((\"Run Tasks Sequentially in Context\"))\n    create_multiple_testbed_contexts --> run_tasks_par_across_contexts((\"Run Tasks in Parallel across Contexts\"))\n    run_tasks_seq_in_context --> task_env_setup((\"Task Environment Setup\"))\n    run_tasks_par_across_contexts --> task_env_setup\n    task_env_setup --> check_skip_task_instance((\"Check if Task Instance Should be Skipped\"))\n    check_skip_task_instance -->|Yes| skip_task_instance((\"Skip Task Instance\"))\n    check_skip_task_instance -->|No| reset_task_env_ev((\"Reset Task Environment\"))\n    reset_task_env_ev --> run_install_task_ev((\"Run Install Task\"))\n    run_install_task_ev --> apply_test_patch((\"Apply Test Patch\"))\n    apply_test_patch --> run_tests_task_test_patch((\"Run Tests Task (Test Patch)\"))\n    run_tests_task_test_patch --> apply_gold_patch((\"Apply Gold Patch\"))\n    apply_gold_patch --> run_tests_task_gold_patch((\"Run Tests Task (Gold Patch)\"))\n    skip_task_instance --> continue_next_task_instance((\"Continue to Next Task Instance\"))\n    run_tests_task_gold_patch --> continue_next_task_instance\n    continue_next_task_instance --> end_ev((\"End\"))\n    end\n\n    subgraph run_evaluation_section[\"run_evaluation.py\"]\n    validate_arguments_re((\"Validate Arguments\")) --> load_tasks((\"Load Tasks\"))\n    load_tasks --> tasks_source((\"Tasks Source\"))\n    tasks_source -->|Local Path| load_from_path((\"Load from Path\"))\n    tasks_source -->|dev/test| load_from_hf_datasets((\"Load from HuggingFace Datasets\"))\n    load_from_path --> verify_tasks_format((\"Verify Tasks Format\"))\n    load_from_hf_datasets --> verify_tasks_format\n    verify_tasks_format --> validate_predictions_re((\"Validate Predictions\"))\n    validate_predictions_re --> group_predictions_by_model((\"Group Predictions by Model\"))\n    group_predictions_by_model --> for_each_model((\"For Each Model\"))\n    for_each_model --> group_predictions_by_repo_and_version((\"Group Predictions by Repo and Version\"))\n    group_predictions_by_repo_and_version --> for_each_repo_version((\"For Each Repo/Version\"))\n    for_each_repo_version --> create_testbed_folder((\"Create Testbed Folder\"))\n    create_testbed_folder --> save_predictions_to_file((\"Save Predictions to File\"))\n    save_predictions_to_file --> skip_existing_checks_re((\"Skip Existing Checks\"))\n    skip_existing_checks_re -->|Yes| filter_already_evaluated_predictions((\"Filter Already Evaluated Predictions\"))\n    skip_existing_checks_re -->|No| proceed_without_filtering((\"Proceed Without Filtering\"))\n    filter_already_evaluated_predictions --> log_info_skip_if_all_exist((\"Log Info & Skip if All Exist\"))\n    proceed_without_filtering --> log_info_skip_if_all_exist\n    log_info_skip_if_all_exist --> any_predictions_to_evaluate((\"Any Predictions to Evaluate?\"))\n    any_predictions_to_evaluate -->|No| log_info_exit((\"Log Info & Exit\"))\n    any_predictions_to_evaluate -->|Yes| prepare_evaluation_arguments((\"Prepare Evaluation Arguments\"))\n    prepare_evaluation_arguments --> run_evaluation_re((\"Run Evaluation\"))\n    run_evaluation_re -->|Single Process| sequential_evaluation((\"Sequential Evaluation\"))\n    run_evaluation_re -->|Multiple Processes| parallel_evaluation((\"Parallel Evaluation\"))\n    sequential_evaluation --> clean_up_re((\"Clean Up\"))\n    parallel_evaluation --> clean_up_re\n    clean_up_re --> end_re((\"End\"))\n    end","metadata":{"source":".docy/docs/markdown/harness/summary.md"}}],["20",{"pageContent":"subgraph run_validation_section[\"run_validation.sh\"]\n    start_ev_script((\"Start engine_validation.py\")) --> check_paths_provided((\"Check if paths are provided\"))\n    check_paths_provided -->|Yes| initialize_with_provided_paths((\"Initialize with provided paths\"))\n    check_paths_provided -->|No| exit_with_error((\"Exit with error: Paths not provided\"))\n    initialize_with_provided_paths --> check_num_workers_rv((\"Check if num_workers is provided\"))\n    check_num_workers_rv -->|Yes| set_number_of_workers((\"Set number of workers\"))\n    check_num_workers_rv -->|No| use_default_number_of_workers((\"Use default number of workers\"))\n    set_number_of_workers --> start_validation_process((\"Start validation process\"))\n    use_default_number_of_workers --> start_validation_process\n    start_validation_process --> is_verbose_mode_on((\"Is verbose mode on?\"))\n    is_verbose_mode_on -->|Yes| run_in_verbose_mode((\"Run in verbose mode\"))\n    is_verbose_mode_on -->|No| run_in_silent_mode((\"Run in silent mode\"))\n    run_in_verbose_mode --> perform_validation_on_task_instances((\"Perform validation on task instances\"))\n    run_in_silent_mode --> perform_validation_on_task_instances\n    perform_validation_on_task_instances --> validation_complete((\"Validation Complete\"))\n    validation_complete --> generate_and_save_logs_to_log_dir((\"Generate and save logs\"))\n    validation_complete --> temporary_files_saved_to_temp_dir((\"Temporary files saved\"))\n    generate_and_save_logs_to_log_dir --> end_of_ev_script((\"End of engine_validation.py\"))\n    temporary_files_saved_to_temp_dir --> end_of_ev_script\n    end\n\n    subgraph utils_section[\"utils.py\"]\n    load_env_vars((\"Load Environment Variables\")) --> get_task_instances_u((\"Get Task Instances\"))\n    get_task_instances_u --> for_each_instance((\"For Each Instance\"))\n    for_each_instance --> get_conda_env_names((\"Get Conda Environment Names\"))\n    get_conda_env_names --> get_environment_yml((\"Get Environment YML\"))\n    get_environment_yml --> get_requirements_txt((\"Get Requirements.txt\"))\n    get_requirements_txt --> get_test_directives((\"Get Test Directives\"))\n    get_test_directives --> clone_repo_u((\"Clone Repo\"))\n    clone_repo_u --> split_instances((\"Split Instances\"))\n    split_instances --> find_python_version_by_date((\"Find Python Version by Date\"))\n    find_python_version_by_date --> extract_minimal_patch((\"Extract Minimal Patch\"))\n    extract_minimal_patch --> check_for_attribute_or_import_error((\"Check for Attribute or Import Error\"))\n    check_for_attribute_or_import_error --> end_u((\"End\"))\n    end\n\n    constants_section --> context_manager_section\n    context_manager_section --> engine_evaluation_section\n    engine_evaluation_section --> engine_validation_section\n    engine_validation_section --> run_evaluation_section\n    run_evaluation_section --> run_validation_section\n    run_validation_section --> utils_section\n```","metadata":{"source":".docy/docs/markdown/harness/summary.md"}}],["21",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Load Environment Variables}\n    B --> C[Get Task Instances]\n    C --> D{For Each Instance}\n    D --> E[Get Conda Environment Names]\n    E --> F[Get Environment YML]\n    F --> G[Get Requirements.txt]\n    G --> H[Get Test Directives]\n    H --> I[Clone Repo]\n    I --> J[Split Instances]\n    J --> K[Find Python Version by Date]\n    K --> L[Extract Minimal Patch]\n    L --> M[Check for Attribute or Import Error]\n    M --> N[End]\n\n    click E \"Get list of conda environment names for given conda path\"\n    click F \"Get environment.yml for given task instance\"\n    click G \"Get requirements.txt for given task instance\"\n    click H \"Get test directives from the test_patch of a task instance\"\n    click I \"Wrapper for cloning repo from swe-bench organization\"\n    click J \"Split a list into n approximately equal length sublists\"\n    click K \"Find python version closest to given date\"\n    click L \"Wrapper function that takes hunk and recalculates hunk start/end position and diff delta\"\n    click M \"Check to see if Attribute/Import-prefix is in log text\"\n```\nThis flowchart represents the sequence of operations performed by the code in the `swe-bench` project file. Each node represents a function or a step in the process, illustrating how the code manages environment variables, interacts with task instances, and processes information related to Python environments, requirements, and repositories.","metadata":{"source":".docy/docs/markdown/harness/utils.md"}}],["22",{"pageContent":"```markdown\n```mermaid\ngraph TD\n    A[Start] --> B[Initialize swe-bench Environment]\n    B --> C{Check if Data Exists}\n    C -->|Yes| D[Load Existing Data]\n    C -->|No| E[Create New Data]\n    D --> F[Process Data]\n    E --> F\n    F --> G{Is Processing Complete?}\n    G -->|Yes| H[Save Processed Data]\n    G -->|No| F\n    H --> I[Generate Reports]\n    I --> J[End]\n```\n```","metadata":{"source":".docy/docs/markdown/inference/__init__.md"}}],["23",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Initialize Model Components]\n    B --> C{Component Type}\n    C -->|Embed Tokens| D[Embedding Layer]\n    C -->|Model Layers| E[Processing Layers]\n    C -->|Norm| F[Normalization Layer]\n    C -->|LM Head| G[Language Model Head]\n    D --> H[Token Embedding Processing]\n    E --> I[Layer-wise Processing]\n    F --> J[Normalization Processing]\n    G --> K[Language Model Output]\n    I --> L{Layer Activation}\n    L -->|0| M[No Activation]\n    L -->|1| N[Activation Level 1]\n    L -->|2| O[Activation Level 2]\n    L -->|3| P[Activation Level 3]\n    M --> Q[Output to Next Layer/Process]\n    N --> Q\n    O --> Q\n    P --> Q\n    Q --> R{Next Component}\n    R -->|Model Layers| E\n    R -->|Norm| F\n    R -->|LM Head| G\n    R -->|End| S[End of Model Processing]\n```\nThis mermaid diagram represents the flow of processing within a model, starting from initialization, through different types of components (embedding, layers, normalization, and language model head), and detailing how each layer's activation level affects the processing flow.","metadata":{"source":".docy/docs/markdown/inference/codellama_device_maps.md"}}],["24",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Define Channels]\n    B --> C[Define Dependencies]\n    C --> D[Base Environment Dependencies]\n    D --> E[Python and Core Libraries]\n    E --> F[Development Tools]\n    F --> G[Define pip Dependencies]\n    G --> H[Core Python Libraries]\n    H --> I[Data Processing and AI Libraries]\n    I --> J[Web and Networking Libraries]\n    J --> K[Development and Debugging Tools]\n    K --> L[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style L fill:#f9f,stroke:#333,stroke-width:4px\n```","metadata":{"source":".docy/docs/markdown/inference/environment.md"}}],["25",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Configuration]\n    B --> C{Check if Configuration is Valid}\n    C -->|Yes| D[Load Test Data]\n    C -->|No| E[Log Error and Exit]\n    D --> F[Initialize Test Environment]\n    F --> G{Is Environment Ready?}\n    G -->|Yes| H[Run Benchmarks]\n    G -->|No| I[Log Error and Retry Initialization]\n    I --> F\n    H --> J[Collect Benchmark Results]\n    J --> K{Are Results Valid?}\n    K -->|Yes| L[Report Success and Store Results]\n    K -->|No| M[Log Failure and Retry Benchmarks]\n    M --> H\n    L --> N[End]\n```\nThis flowchart represents the logical steps taken by the code in the `swe-bench` project. It starts with initializing the configuration, followed by validating it. If the configuration is invalid, it logs an error and exits. Upon successful validation, it loads test data and initializes the test environment. If the environment is not ready, it logs an error and retries initialization. Once the environment is ready, it runs benchmarks, collects results, and checks their validity. If the results are valid, it reports success and stores the results. If not, it logs a failure and retries the benchmarks.","metadata":{"source":".docy/docs/markdown/inference/llamao/__init__.md"}}],["26",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Model Components]\n    B --> C{Is it Training?}\n    C -->|Yes| D[Gradient Checkpointing]\n    C -->|No| E[Process Input Embeddings]\n    E --> F[Unpad Input if Required]\n    F --> G[Iterate Over Decoder Layers]\n    G --> H{Is Output Attention?}\n    H -->|Yes| I[Output Attention Weights]\n    H -->|No| J[No Attention Output]\n    I --> K[Post Layer Operations]\n    J --> K\n    K --> L{Is Output Hidden States?}\n    L -->|Yes| M[Output Hidden States]\n    L -->|No| N[No Hidden States Output]\n    M --> O[Apply Final Layer Norm]\n    N --> O\n    O --> P{Is it Sequence Classification?}\n    P -->|Yes| Q[Apply Classification Head]\n    P -->|No| R{Is it Causal LM?}\n    R -->|Yes| S[Apply LM Head]\n    R -->|No| T[Other Task Processing]\n    Q --> U[End]\n    S --> U\n    T --> U\n```\nThis flowchart represents the general workflow of the code provided, focusing on the initialization of model components, handling of input embeddings, processing through decoder layers, and the application of specific heads based on the task (e.g., sequence classification or causal language modeling). It also highlights conditional paths such as gradient checkpointing during training, outputting attention weights, and hidden states based on configuration flags.","metadata":{"source":".docy/docs/markdown/inference/llamao/modeling_flash_llama.md"}}],["27",{"pageContent":"```mermaid\nflowchart TD\n    subgraph swe_bench [\" \"]\n    start((Start)) --> init_conf[Initialize Configuration]\n    init_conf --> check_conf{Check if Configuration is Valid}\n    check_conf -->|Yes| load_data[Load Test Data]\n    check_conf -->|No| log_error1[Log Error and Exit]\n    load_data --> init_test_env[Initialize Test Environment]\n    init_test_env --> env_ready{Is Environment Ready?}\n    env_ready -->|Yes| run_benchmarks[Run Benchmarks]\n    env_ready -->|No| log_error2[Log Error and Retry Initialization]\n    log_error2 --> init_test_env\n    run_benchmarks --> collect_results[Collect Benchmark Results]\n    collect_results --> results_valid{Are Results Valid?}\n    results_valid -->|Yes| report_success[Report Success and Store Results]\n    results_valid -->|No| log_failure[Log Failure and Retry Benchmarks]\n    log_failure --> run_benchmarks\n    report_success --> end((End))\n    \n    subgraph modeling_flash_llama [\" \"]\n    start_modeling((Start)) --> init_model_comp[Initialize Model Components]\n    init_model_comp --> is_training{Is it Training?}\n    is_training -->|Yes| grad_checkpoint[Gradient Checkpointing]\n    is_training -->|No| proc_input_emb[Process Input Embeddings]\n    proc_input_emb --> unpad_input[Unpad Input if Required]\n    unpad_input --> iterate_dec_layers[Iterate Over Decoder Layers]\n    iterate_dec_layers --> is_out_att{Is Output Attention?}\n    is_out_att -->|Yes| out_att_weights[Output Attention Weights]\n    is_out_att -->|No| no_att_output[No Attention Output]\n    out_att_weights --> post_layer_ops[Post Layer Operations]\n    no_att_output --> post_layer_ops\n    post_layer_ops --> is_out_hidden{Is Output Hidden States?}\n    is_out_hidden -->|Yes| out_hidden_states[Output Hidden States]\n    is_out_hidden -->|No| no_hidden_output[No Hidden States Output]\n    out_hidden_states --> final_layer_norm[Apply Final Layer Norm]\n    no_hidden_output --> final_layer_norm\n    final_layer_norm --> is_seq_class{Is it Sequence Classification?}\n    is_seq_class -->|Yes| apply_class_head[Apply Classification Head]\n    is_seq_class -->|No| is_causal_lm{Is it Causal LM?}\n    is_causal_lm -->|Yes| apply_lm_head[Apply LM Head]\n    is_causal_lm -->|No| other_task_proc[Other Task Processing]\n    apply_class_head --> end_modeling((End))\n    apply_lm_head --> end_modeling\n    other_task_proc --> end_modeling\n    end_modeling --> end\n    end\n    end\n    \n    swe_bench --> modeling_flash_llama\n```\n\nThis mermaid markdown code illustrates the workflow within the `swe-bench` project, specifically focusing on the `.docy/docs/json/inference/llamao` folder. It starts with the initialization and validation of the configuration, followed by the loading of test data and the initialization of the test environment. Upon ensuring the environment is ready, it proceeds to run benchmarks, collect results, and validate them. Parallelly, it details the workflow of `modeling_flash_llama.py`, which includes model component initialization, processing input embeddings, iterating over decoder layers, and applying specific heads based on the task. The diagram also shows how `modeling_flash_llama.py` fits into the larger workflow, indicating that the modeling process is a part of the benchmarking process, potentially contributing to the benchmarks run in the project.","metadata":{"source":".docy/docs/markdown/inference/llamao/summary.md"}}],["28",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Benchmark Environment]\n    B --> C{Check if Benchmark Data Exists}\n    C -->|Yes| D[Load Existing Benchmark Data]\n    C -->|No| E[Generate New Benchmark Data]\n    D --> F[Prepare Benchmark Tests]\n    E --> F\n    F --> G{Is Benchmark Configuration Valid?}\n    G -->|Yes| H[Run Benchmark Tests]\n    G -->|No| I[Log Configuration Error]\n    H --> J[Collect Benchmark Results]\n    J --> K{Are Results Within Expected Range?}\n    K -->|Yes| L[Store Benchmark Results]\n    K -->|No| M[Log Results Out of Range Error]\n    L --> N[Generate Benchmark Report]\n    M --> N\n    N --> O[End]\n```\nThis flowchart represents the steps executed by the code in the `swe-bench` project. It starts with initializing the benchmark environment, followed by checking for existing benchmark data. Depending on whether the data exists, it either loads the existing data or generates new data. After preparing the benchmark tests, it checks if the benchmark configuration is valid. If valid, it runs the benchmark tests; otherwise, it logs a configuration error. Post running the tests, it collects the results and checks if they are within the expected range. If the results are as expected, it stores them and generates a report. If not, it logs an error indicating the results are out of the expected range before generating the report. The process ends after the report generation.","metadata":{"source":".docy/docs/markdown/inference/make_datasets/__init__.md"}}],["29",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Check if dataset path exists}\n    B -- Yes --> C[Load dataset from disk]\n    B -- No --> D[Load dataset from HuggingFace Datasets]\n    C --> E[Prepare dataset splits]\n    D --> E\n    E --> F{Check for unknown splits}\n    F -- Yes --> G[Error: Unknown splits]\n    F -- No --> H[Get remaining instances]\n    H --> I[Get root directory and name]\n    I --> J[Get index paths for instances]\n    J --> K[Search indexes for instances]\n    K --> L[Get missing IDs]\n    L --> M[Cleanup and finish]\n    M --> N[End]\n```\n\nThis flowchart represents the process outlined in the provided code, focusing on the main steps involved in preparing, processing, and cleaning up after searching indexes for instances in a dataset. The process starts with checking if the dataset path exists, leading to either loading the dataset from disk or from HuggingFace Datasets. After preparing the dataset splits, it checks for unknown splits, which could lead to an error. Then, it proceeds to get the remaining instances that need processing, prepares the root directory, retrieves index paths for instances, and performs searches on these indexes. Finally, it identifies any missing IDs, cleans up resources, and finishes the process.","metadata":{"source":".docy/docs/markdown/inference/make_datasets/bm25_retrieval.md"}}],["30",{"pageContent":"```mermaid\ngraph TD\n    A(Start) --> B[Import Libraries and Modules]\n    B --> C[Define Constants and Load External Resources]\n    C --> D[Define Utility Functions]\n    D --> E{Check for File Source}\n    E -->|oracle| F[Ingest Files from Oracle Filenames]\n    E -->|bm25| G[Add Retrieval Results]\n    E -->|all| H[Ingest All Directory Contents]\n    E -->|none| I[Set File Contents to Empty]\n    G --> J[Ingest Files from Retrieval Hits]\n    F --> K[Generate Prompt Based on Style]\n    J --> K\n    H --> K\n    I --> K\n    K --> L[Add Text Inputs to Instances]\n    L --> M[Handle Context Length Limitation]\n    M --> N[Finalize Text Inputs]\n    N --> O[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style O fill:#f9f,stroke:#333,stroke-width:4px\n```","metadata":{"source":".docy/docs/markdown/inference/make_datasets/create_instance.md"}}],["31",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Parse Arguments]\n    B --> C{Check push_to_hub_user}\n    C -->|None| D[Create output directory if not exists]\n    C -->|Not None| E[Check HUGGING_FACE_HUB_TOKEN and output_dir]\n    B --> F{Check max_context_len}\n    F -->|Not None| G[Assert tokenizer_name is not None]\n    B --> H[Generate output file name based on arguments]\n    H --> I{Check if dataset exists on disk}\n    I -->|Yes| J[Load dataset from disk]\n    I -->|No| K[Load dataset from HuggingFace Datasets]\n    K --> L[Process each split in dataset]\n    L --> M[Add text inputs to instances]\n    M --> N[Extract fields from instances]\n    N --> O[Create Dataset from processed data]\n    O --> P{Check validation_ratio}\n    P -->|>0| Q[Split train dataset into train and validation]\n    P -->|<=0| R[Proceed without splitting]\n    Q --> S[Log number of instances per split]\n    R --> S\n    S --> T{Check push_to_hub_user}\n    T -->|Not None| U[Push dataset to HuggingFace Hub]\n    T -->|None| V[Save dataset to disk]\n    U --> W[End]\n    V --> W\n```\nThis mermaid diagram illustrates the flow of operations in the provided code snippet. It starts with parsing command-line arguments, checks conditions related to `push_to_hub_user` and `max_context_len`, and decides on the dataset loading strategy. It then processes each dataset split by adding text inputs, extracting necessary fields, and creating a `Dataset` object. Depending on the `validation_ratio`, it may split the training dataset. Finally, it either pushes the dataset to the HuggingFace Hub or saves it to disk, based on the `push_to_hub_user` argument.","metadata":{"source":".docy/docs/markdown/inference/make_datasets/create_text_dataset.md"}}],["32",{"pageContent":"```mermaid\nflowchart TB\n    subgraph swe_bench [\" \"]\n    start((Start))\n    init[(\"/__init__.py\\nInitialize Benchmark Environment\")]\n    bm25[(\"/bm25_retrieval.py\\nBM25 Dataset Retrieval\")]\n    create_inst[(\"/create_instance.py\\nCreate Instance\")]\n    create_text_ds[(\"/create_text_dataset.py\\nCreate Text Dataset\")]\n    tokenize_ds[(\"/tokenize_dataset.py\\nTokenize Dataset\")]\n    utils[(\"/utils.py\\nUtility Functions\")]\n    end((End))\n\n    start --> init\n    init --> bm25\n    init --> create_inst\n    init --> create_text_ds\n    init --> tokenize_ds\n    init --> utils\n    bm25 --> create_text_ds\n    create_inst --> create_text_ds\n    tokenize_ds --> create_text_ds\n    utils --> create_inst\n    utils --> bm25\n    create_text_ds --> end\n    end\n    end\n```\n\nThis diagram illustrates the interconnected steps and processes within the `make_datasets` folder of the project. It starts with initializing the benchmark environment, followed by various processes such as BM25 dataset retrieval, instance creation, text dataset creation, dataset tokenization, and utility functions. Each script contributes to the preparation and processing of datasets, ultimately leading to the creation of a text dataset that is ready for further actions, such as benchmarking or analysis. Utility functions support various steps in the process, indicating a cohesive workflow aimed at dataset preparation and processing within the project.","metadata":{"source":".docy/docs/markdown/inference/make_datasets/summary.md"}}],["33",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Check if tokenizer name provided}\n    B -->|Yes| C[Initialize tokenizer and function]\n    B -->|No| Z[End without tokenizer initialization]\n    C --> D{Check if dataset path exists}\n    D -->|Yes| E[Load dataset from disk]\n    D -->|No| F[Load dataset using name or path]\n    E & F --> G[Filter out superlong instances]\n    G --> H{Is split test?}\n    H -->|No| I[Tokenize non-test split]\n    H -->|Yes| J[Skip tokenizing test split]\n    I --> K{Is num_proc > 0?}\n    K -->|Yes| L[Tokenize with multiprocessing]\n    K -->|No| M[Tokenize without multiprocessing]\n    L & M --> N[Add tokenized columns to dataset]\n    J --> O{Is num_proc > 0?}\n    O -->|Yes| P[Tokenize test split with multiprocessing]\n    O -->|No| Q[Tokenize test split without multiprocessing]\n    P & Q --> R[Add tokenized columns to test dataset]\n    N & R --> S{Is push to hub user specified?}\n    S -->|Yes| T[Push dataset to Hugging Face Hub]\n    S -->|No| U[Save dataset to disk]\n    T & U --> Z[End]\n```","metadata":{"source":".docy/docs/markdown/inference/make_datasets/tokenize_dataset.md"}}],["34",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Extract Diff]\n    B --> C{Is Test?}\n    C -->|Yes| D[Skip File]\n    C -->|No| E[Detect Encoding]\n    E --> F[Read File Content]\n    F --> G[Ingest Directory Contents]\n    G --> H[Ingest File Directory Contents]\n    H --> I[Repair Patch]\n    I --> J[Extract Minimal Patch]\n    J --> K[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style K fill:#f9f,stroke:#333,stroke-width:4px\n```\nThis flowchart represents the sequence of operations performed by the code in the provided context. It starts with extracting diffs from a given input, checks if the file is a test file, detects the file encoding, reads the file content, ingests directory contents, ingests file directory contents, repairs patches, and finally extracts a minimal patch from the given input. The process begins at \"Start\" and ends at \"End\", with decision points and operations represented as nodes in the flowchart.","metadata":{"source":".docy/docs/markdown/inference/make_datasets/utils.md"}}],["35",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Check Model Prefix}\n    B -->|claude| C[Anthropic Inference]\n    B -->|gpt| D[OpenAI Inference]\n    C --> E[Load Dataset]\n    D --> E\n    E --> F{Filter Dataset}\n    F -->|Existing IDs| G[Filter out existing IDs]\n    F -->|Shard Info| H[Select Shard]\n    G --> H\n    H --> I[Inference Loop]\n    I --> J{Check Max Cost}\n    J -->|Reached| K[Exit]\n    J -->|Not Reached| I\n    K --> L[End]\n```\n\nThis flowchart represents the high-level logic of the provided code. The process starts with determining the model prefix to decide whether to use the Anthropic or OpenAI inference path. After loading and optionally filtering the dataset based on existing IDs and shard information, it enters an inference loop. Within this loop, it checks if the maximum cost has been reached after each inference call. If the max cost is reached, the process exits; otherwise, it continues with the next inference call until completion.","metadata":{"source":".docy/docs/markdown/inference/run_api.md"}}],["36",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Parse Issue URL}\n    B -->|Valid| C[Get Problem Statement]\n    B -->|Invalid| Z[Error: Invalid URL]\n    C --> D[Clone Repository]\n    D --> E[Build BM25 Retrieval Index]\n    E --> F[Search in Index]\n    F --> G{Include READMEs?}\n    G -->|Yes| H[Get README Files]\n    G -->|No| I[Skip READMEs]\n    H --> J[Ingest Files]\n    I --> J\n    J --> K[Generate Prompt]\n    K --> L[Call Model]\n    L --> M[Extract Diff from Response]\n    M --> N[Extract Minimal Patch]\n    N --> O[Save Output]\n    O --> P[End]\n```\nThis flowchart represents the steps taken by the code to run a live inference session on a GitHub issue. It starts with parsing the issue URL, retrieving the problem statement, cloning the repository, and building a BM25 retrieval index. It then searches the index, decides whether to include README files, ingests files if necessary, and generates a prompt. The model is called with this prompt, and the response is processed to extract a diff and a minimal patch. Finally, the output is saved, and the process ends.","metadata":{"source":".docy/docs/markdown/inference/run_live.md"}}],["37",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Check Shard Info}\n    B -->|Valid| C[Load PEFT Config]\n    B -->|Invalid| D[Error: Shard Info Mismatch]\n    C --> E[Construct Output File Path]\n    E --> F[Load Model]\n    F --> G[Load Tokenizer]\n    G --> H[Get All Existing IDs]\n    H --> I[Load and Preprocess Data]\n    I --> J[Generate Patches]\n    J --> K[End]\n\n    click B \"Check if shard_id and num_shards are specified correctly.\"\n    click C \"Load PEFT configuration if peft_path is provided.\"\n    click E \"Construct the output file path based on parameters.\"\n    click F \"Load the model with or without PEFT adapters.\"\n    click G \"Load the tokenizer for the model.\"\n    click H \"Get all existing instance IDs from the output file to avoid duplicates.\"\n    click I \"Load the dataset, tokenize, filter, and shard it.\"\n    click J \"Generate patches for the dataset using the model.\"\n```","metadata":{"source":".docy/docs/markdown/inference/run_llama.md"}}],["38",{"pageContent":"```mermaid\nflowchart TB\n    subgraph inference_process [\"Inference Process\"]\n        start((Start)) --> init_env[Initialize swe-bench Environment]\n        init_env --> data_check{Check if Data Exists}\n        data_check -->|Yes| load_data[Load Existing Data]\n        data_check -->|No| create_data[Create New Data]\n        load_data --> process_data[Process Data]\n        create_data --> process_data\n        process_data --> processing_complete{Is Processing Complete?}\n        processing_complete -->|Yes| save_data[Save Processed Data]\n        processing_complete -->|No| process_data\n        save_data --> generate_reports[Generate Reports]\n        generate_reports --> end((End))\n    end\n\n    subgraph model_processing [\"Model Processing\"]\n        model_start((Start)) --> init_model[Initialize Model Components]\n        init_model --> component_type{Component Type}\n        component_type -->|Embed Tokens| embedding_layer[Embedding Layer]\n        component_type -->|Model Layers| processing_layers[Processing Layers]\n        component_type -->|Norm| normalization_layer[Normalization Layer]\n        component_type -->|LM Head| language_model_head[Language Model Head]\n        embedding_layer --> token_embedding_proc[Token Embedding Processing]\n        processing_layers --> layer_wise_proc[Layer-wise Processing]\n        normalization_layer --> normalization_proc[Normalization Processing]\n        language_model_head --> language_model_output[Language Model Output]\n        layer_wise_proc --> layer_activation{Layer Activation}\n        layer_activation -->|0| no_activation[No Activation]\n        layer_activation -->|1| activation_lvl1[Activation Level 1]\n        layer_activation -->|2| activation_lvl2[Activation Level 2]\n        layer_activation -->|3| activation_lvl3[Activation Level 3]\n        no_activation --> output_next_layer[Output to Next Layer/Process]\n        activation_lvl1 --> output_next_layer\n        activation_lvl2 --> output_next_layer\n        activation_lvl3 --> output_next_layer\n        output_next_layer --> next_component{Next Component}\n        next_component -->|Model Layers| processing_layers\n        next_component -->|Norm| normalization_layer\n        next_component -->|LM Head| language_model_head\n        next_component -->|End| model_end((End of Model Processing))\n    end\n\n    subgraph environment_setup [\"Environment Setup\"]\n        env_start((Start)) --> define_channels[Define Channels]\n        define_channels --> define_dependencies[Define Dependencies]\n        define_dependencies --> base_env_deps[Base Environment Dependencies]\n        base_env_deps --> python_core_libs[Python and Core Libraries]\n        python_core_libs --> dev_tools[Development Tools]\n        dev_tools --> define_pip_deps[Define pip Dependencies]\n        define_pip_deps --> core_python_libs[Core Python Libraries]\n        core_python_libs --> data_ai_libs[Data Processing and AI Libraries]\n        data_ai_libs --> web_net_libs[Web and Networking Libraries]\n        web_net_libs --> dev_debug_tools[Development and Debugging Tools]\n        dev_debug_tools --> env_end((End))\n    end\n\n    subgraph api_inference [\"API Inference\"]\n        api_start((Start)) --> check_model_prefix{Check Model Prefix}\n        check_model_prefix -->|claude| anthropic_inference[Anthropic Inference]\n        check_model_prefix -->|gpt| openai_inference[OpenAI Inference]\n        anthropic_inference --> load_dataset[Load Dataset]\n        openai_inference --> load_dataset\n        load_dataset --> filter_dataset{Filter Dataset}\n        filter_dataset -->|Existing IDs| filter_existing_ids[Filter out existing IDs]\n        filter_dataset -->|Shard Info| select_shard[Select Shard]\n        filter_existing_ids --> select_shard\n        select_shard --> inference_loop[Inference Loop]\n        inference_loop --> check_max_cost{Check Max Cost}\n        check_max_cost -->|Reached| exit[Exit]\n        check_max_cost -->|Not Reached| inference_loop\n        exit --> api_end((End))\n    end\n\n    subgraph live_inference [\"Live Inference\"]\n        live_start((Start)) --> parse_issue_url{Parse Issue URL}\n        parse_issue_url -->|Valid| get_problem_statement[Get Problem Statement]\n        parse_issue_url -->|Invalid| error_invalid_url[Error: Invalid URL]\n        get_problem_statement --> clone_repo[Clone Repository]\n        clone_repo --> build_bm25_index[Build BM25 Retrieval Index]\n        build_bm25_index --> search_index[Search in Index]\n        search_index --> include_readmes{Include READMEs?}\n        include_readmes -->|Yes| get_readme_files[Get README Files]\n        include_readmes -->|No| skip_readmes[Skip READMEs]\n        get_readme_files --> ingest_files[Ingest Files]\n        skip_readmes --> ingest_files\n        ingest_files --> generate_prompt[Generate Prompt]\n        generate_prompt --> call_model[Call Model]\n        call_model --> extract_diff[Extract Diff from Response]\n        extract_diff --> extract_minimal_patch[Extract Minimal Patch]\n        extract_minimal_patch --> save_output[Save Output]\n        save_output --> live_end((End))\n    end\n\n    subgraph dataset_preparation [\"Dataset Preparation\"]\n        ds_start((Start)) --> init_bench_env[Initialize Benchmark Environment]\n        init_bench_env --> bm25_retrieval[BM25 Dataset Retrieval]\n        init_bench_env --> create_instance[Create Instance]\n        init_bench_env --> create_text_dataset[Create Text Dataset]\n        init_bench_env --> tokenize_dataset[Tokenize Dataset]\n        init_bench_env --> util_funcs[Utility Functions]\n        bm25_retrieval --> create_text_dataset\n        create_instance --> create_text_dataset\n        tokenize_dataset --> create_text_dataset\n        util_funcs --> create_instance\n        util_funcs --> bm25_retrieval\n        create_text_dataset --> ds_end((End))\n    end\n\n    inference_process --> model_processing --> environment_setup --> api_inference --> live_inference --> dataset_preparation\n```","metadata":{"source":".docy/docs/markdown/inference/summary.md"}}],["39",{"pageContent":"```mermaid\nflowchart TD\n    A[Start convert_log_to_ground_truth] --> B{Check if log file can be parsed}\n    B -- Yes --> C[Retrieve Instance File Name and Repository]\n    B -- No --> Z[Error: Log file could not be parsed]\n    C --> D[Identify Log Parser based on Repository]\n    D --> E[Extract Before and After Test Status]\n    E --> F{Loop through After Test Status}\n    F -->|Test Passed| G[Test Passed in Before Status]\n    F -->|Test Failed| H[Test Failed in Before Status]\n    G --> I{Was Test Initially Passing?}\n    H --> J{Was Test Initially Failing?}\n    I -- Yes --> K[Add to PASS_TO_PASS]\n    I -- No --> L[Add to FAIL_TO_PASS]\n    J -- Yes --> M[Add to PASS_TO_FAIL]\n    J -- No --> N[Add to FAIL_TO_FAIL]\n    K & L & M & N --> O{Is Save Directory Provided?}\n    O -- Yes --> P[Save Results to File]\n    O -- No --> Q[Return Status Ground Truth]\n    P --> Q[Return Status Ground Truth]\n    Z --> Q[Return Status Ground Truth]\n```\nThis mermaid markdown code illustrates the steps involved in the `convert_log_to_ground_truth` function from the provided code snippet. It starts with checking if the log file can be parsed, then moves through identifying the repository, selecting the appropriate log parser, extracting before and after test statuses, categorizing tests based on their transitions between statuses, and optionally saving the results to a file if a save directory is provided.","metadata":{"source":".docy/docs/markdown/metrics/conversion.md"}}],["40",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Get Log File Path}\n    B --> C[Identify Repository]\n    C --> D{Check Log Type}\n    D -->|Evaluation Log| E[Check Patch Application Success]\n    D -->|Validation Log| F[Split Pre and Post Patch Logs]\n    E -->|Success| G[Parse Evaluation Results]\n    E -->|Fail| H[Return Empty Status Map, False]\n    F -->|Found 2 Patches| I[Parse Pre and Post Patch Logs]\n    F -->|Less than 2 Patches| J[Return None, None]\n    G --> K[Return Status Map, True]\n    I --> L{Attempt Parsing Logs}\n    L -->|Success| M[Return Status Maps List, True]\n    L -->|Fail| N[Return None, False]\n    M --> O[End]\n    N --> O\n    K --> O\n    J --> O\n    H --> O\n```\nThis flowchart represents the process of handling log files in the `swe-bench` project. It starts with obtaining the log file path and identifying the repository associated with it. Based on the log type, it either checks for patch application success in evaluation logs or splits the log into pre and post-patch logs for validation logs. For evaluation logs, if the patch application is successful, it parses the evaluation results; otherwise, it returns an empty status map and `False`. For validation logs, if two patches are found, it attempts to parse the pre and post-patch logs; otherwise, it returns `None` for both. If parsing is successful, it returns a list of status maps and `True`; if not, it returns `None` and `False`. The process ends after handling the logs accordingly.","metadata":{"source":".docy/docs/markdown/metrics/getters.md"}}],["41",{"pageContent":"```mermaid\nflowchart TB\n    start[Start] --> selectFramework{Select Testing Framework}\n    selectFramework --> pytest[PyTest]\n    selectFramework --> django[Django Tester]\n    selectFramework --> pytest_v2[PyTest v2]\n    selectFramework --> seaborn[Seaborn]\n    selectFramework --> sympy[Sympy]\n\n    pytest --> parse_pytest[Parse PyTest Log]\n    django --> parse_django[Parse Django Log]\n    pytest_v2 --> parse_pytest_v2[Parse PyTest v2 Log]\n    seaborn --> parse_seaborn[Parse Seaborn Log]\n    sympy --> parse_sympy[Parse Sympy Log]\n\n    parse_pytest --> mapStatusPytest{Map Status}\n    parse_django --> mapStatusDjango{Map Status}\n    parse_pytest_v2 --> mapStatusPytest_v2{Map Status}\n    parse_seaborn --> mapStatusSeaborn{Map Status}\n    parse_sympy --> mapStatusSympy{Map Status}\n\n    mapStatusPytest --> end[End]\n    mapStatusDjango --> end\n    mapStatusPytest_v2 --> end\n    mapStatusSeaborn --> end\n    mapStatusSympy --> end\n\n    classDef framework fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef action fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef mapping fill:#fb4,stroke:#333,stroke-width:2px;\n    class selectFramework framework;\n    class parse_pytest,parse_django,parse_pytest_v2,parse_seaborn,parse_sympy action;\n    class mapStatusPytest,mapStatusDjango,mapStatusPytest_v2,mapStatusSeaborn,mapStatusSympy mapping;\n```\nThis flowchart represents the process of selecting a testing framework and parsing its log to map test cases to their statuses. Each testing framework (PyTest, Django Tester, PyTest v2, Seaborn, and Sympy) has a specific parsing function that processes the log and maps the status of each test case.","metadata":{"source":".docy/docs/markdown/metrics/log_parsers.md"}}],["42",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Compute Metrics]\n    B --> C{Is it a single report?}\n    C -->|Yes| D[Compute Single Report Metrics]\n    C -->|No| E[Compute Multiple Reports Metrics]\n    D --> D1[Compute Fail-to-Pass for Single Report]\n    D --> D2[Compute Pass-to-Pass for Single Report]\n    E --> E1{Is it weighted?}\n    E1 -->|Yes| E2[Compute Weighted Metrics]\n    E1 -->|No| E3[Compute Unweighted Metrics]\n    E2 --> E2A[Compute Weighted Fail-to-Pass]\n    E2 --> E2B[Compute Weighted Pass-to-Pass]\n    E3 --> E3A[Compute Unweighted Fail-to-Pass]\n    E3 --> E3B[Compute Unweighted Pass-to-Pass]\n    D1 & D2 & E2A & E2B & E3A & E3B --> F[Get Resolution Status]\n    F --> G{Determine Status}\n    G -->|FULL| H[Resolved Full]\n    G -->|PARTIAL| I[Resolved Partial]\n    G -->|NO| J[Resolved No]\n    H & I & J --> K[End]\n```\nThis mermaid diagram illustrates the flow of operations within the provided code. It starts with the decision of whether the input is a single report or multiple reports, followed by further decisions based on the type of computation (weighted or unweighted for multiple reports) and concludes with determining the resolution status based on the computed metrics.","metadata":{"source":".docy/docs/markdown/metrics/metrics.md"}}],["43",{"pageContent":"```mermaid\ngraph TD\n    A[Start monitor_validation] --> B[Iterate through log files]\n    B --> C{Check if patch applied}\n    C -->|Yes| D[Count patch applies]\n    D --> E{Check for TESTS_TIMEOUT}\n    E -->|Yes| F[Append to timeout list]\n    E -->|No| G{Check patch applies}\n    G -->|0| H[Append to corrupt_test_patch list]\n    G -->|1| I[Append to corrupt_patch list]\n    G -->|2| J[Append to success list]\n    C -->|No| K[Append to failed_install list]\n    L[End monitor_validation] <-- F\n    L <-- H\n    L <-- I\n    L <-- J\n    L <-- K\n    L --> M[Log results]\n    M --> N[Assert all instances accounted for]\n    N --> O[Return lists]\n\n    P[Start monitor_logs_same_diff] --> Q[Iterate through log files]\n    Q --> R{Check if repo provided}\n    R -->|Yes| S[Use provided repo]\n    R -->|No| T[Get repo from log path]\n    S --> U[Get log parser]\n    T --> U\n    U --> V[Get pre, post patch behavior]\n    V --> W{Check if behavior is same}\n    W -->|Yes| X[Append to logs_same list]\n    W -->|No| Y[Append to logs_diff list]\n    Z[End monitor_logs_same_diff] <-- X\n    Z <-- Y\n    Z --> AA[Return lists]\n```\nThis mermaid diagram illustrates the flow and decision-making process within the `monitor_validation` and `monitor_logs_same_diff` functions. It shows how log files are processed, analyzed for specific conditions (such as patch application success, timeouts, and pre/post patch behavior differences), and categorized accordingly.","metadata":{"source":".docy/docs/markdown/metrics/monitor.md"}}],["44",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Load Evaluation Logs}\n    B --> C{Load Gold Results}\n    C --> D{Calculate Metrics}\n    D --> E{Generate Report}\n    E --> F{Generate Summary}\n    F --> G[End]\n\n    subgraph get_eval_report\n        D -->|Fail to Pass| F2P[Calculate F2P Metrics]\n        D -->|Pass to Pass| P2P[Calculate P2P Metrics]\n        D -->|Fail to Fail| F2F[Calculate F2F Metrics]\n        D -->|Pass to Fail| P2F[Calculate P2F Metrics]\n    end\n\n    subgraph get_eval_reports_for_logs\n        B -->|For Each Log| C\n        C -->|Find Corresponding Gold Result| D\n        D -->|For Each Test Case| E\n    end\n\n    subgraph get_eval_reports_for_dir\n        A -->|Load Logs from Directory| B\n    end\n\n    subgraph get_model_eval_summary\n        A -->|Load Predictions| preds[Load Predictions]\n        preds -->|Filter by Repo| filter[Filter Predictions]\n        filter -->|Get Reports| get_eval_reports_for_dir\n        get_eval_reports_for_dir -->|Compile Summary| F\n    end\n\n    subgraph get_model_report\n        A -->|Load Predictions| predictions[Load Predictions]\n        predictions -->|For Each Prediction| check[Check Model Patch]\n        check -->|Exists| exists[Model Patch Exists]\n        exists -->|Log Exists| log_exists[Log File Exists]\n        log_exists -->|Eval Log Found| found[Eval Log Found]\n        found -->|Generate Report| get_eval_report\n        get_eval_report -->|Resolution Status| resolution[Check Resolution Status]\n        resolution -->|Compile Report| G\n    end\n```\nThis mermaid diagram illustrates the flow and functionalities of the provided code snippets, focusing on the evaluation report generation, model evaluation summary, and model report generation processes within the larger project context.","metadata":{"source":".docy/docs/markdown/metrics/report.md"}}],["45",{"pageContent":"```mermaid\nflowchart TB\n    subgraph conversion [\"conversion.py\"]\n        A[Start convert_log_to_ground_truth] --> B{Check if log file can be parsed}\n        B -- Yes --> C[Retrieve Instance File Name and Repository]\n        B -- No --> Z[Error: Log file could not be parsed]\n        C --> D[Identify Log Parser based on Repository]\n        D --> E[Extract Before and After Test Status]\n        E --> F{Loop through After Test Status}\n        F -->|Test Passed| G[Test Passed in Before Status]\n        F -->|Test Failed| H[Test Failed in Before Status]\n        G --> I{Was Test Initially Passing?}\n        H --> J{Was Test Initially Failing?}\n        I -- Yes --> K[Add to PASS_TO_PASS]\n        I -- No --> L[Add to FAIL_TO_PASS]\n        J -- Yes --> M[Add to PASS_TO_FAIL]\n        J -- No --> N[Add to FAIL_TO_FAIL]\n        K & L & M & N --> O{Is Save Directory Provided?}\n        O -- Yes --> P[Save Results to File]\n        O -- No --> Q[Return Status Ground Truth]\n        P --> Q[Return Status Ground Truth]\n        Z --> Q[Return Status Ground Truth]\n    end\n\n    subgraph getters [\"getters.py\"]\n        A1[Start] --> B1{Get Log File Path}\n        B1 --> C1[Identify Repository]\n        C1 --> D1{Check Log Type}\n        D1 -->|Evaluation Log| E1[Check Patch Application Success]\n        D1 -->|Validation Log| F1[Split Pre and Post Patch Logs]\n        E1 -->|Success| G1[Parse Evaluation Results]\n        E1 -->|Fail| H1[Return Empty Status Map, False]\n        F1 -->|Found 2 Patches| I1[Parse Pre and Post Patch Logs]\n        F1 -->|Less than 2 Patches| J1[Return None, None]\n        G1 --> K1[Return Status Map, True]\n        I1 --> L1{Attempt Parsing Logs}\n        L1 -->|Success| M1[Return Status Maps List, True]\n        L1 -->|Fail| N1[Return None, False]\n        M1 --> O1[End]\n        N1 --> O1\n        K1 --> O1\n        J1 --> O1\n        H1 --> O1\n    end\n\n    subgraph log_parsers [\"log_parsers.py\"]\n        start1[Start] --> selectFramework{Select Testing Framework}\n        selectFramework --> pytest[PyTest]\n        selectFramework --> django[Django Tester]\n        selectFramework --> pytest_v2[PyTest v2]\n        selectFramework --> seaborn[Seaborn]\n        selectFramework --> sympy[Sympy]\n        pytest --> parse_pytest[Parse PyTest Log]\n        django --> parse_django[Parse Django Log]\n        pytest_v2 --> parse_pytest_v2[Parse PyTest v2 Log]\n        seaborn --> parse_seaborn[Parse Seaborn Log]\n        sympy --> parse_sympy[Parse Sympy Log]\n        parse_pytest --> mapStatusPytest{Map Status}\n        parse_django --> mapStatusDjango{Map Status}\n        parse_pytest_v2 --> mapStatusPytest_v2{Map Status}\n        parse_seaborn --> mapStatusSeaborn{Map Status}\n        parse_sympy --> mapStatusSympy{Map Status}\n        mapStatusPytest --> end1[End]\n        mapStatusDjango --> end1\n        mapStatusPytest_v2 --> end1\n        mapStatusSeaborn --> end1\n        mapStatusSympy --> end1\n    end\n\n    subgraph metrics [\"metrics.py\"]\n        A2[Start] --> B2[Compute Metrics]\n        B2 --> C2{Is it a single report?}\n        C2 -->|Yes| D2[Compute Single Report Metrics]\n        C2 -->|No| E2[Compute Multiple Reports Metrics]\n        D2 --> D12[Compute Fail-to-Pass for Single Report]\n        D2 --> D22[Compute Pass-to-Pass for Single Report]\n        E2 --> E12{Is it weighted?}\n        E12 -->|Yes| E22[Compute Weighted Metrics]\n        E12 -->|No| E32[Compute Unweighted Metrics]\n        E22 --> E2A[Compute Weighted Fail-to-Pass]\n        E22 --> E2B[Compute Weighted Pass-to-Pass]\n        E32 --> E3A[Compute Unweighted Fail-to-Pass]\n        E32 --> E3B[Compute Unweighted Pass-to-Pass]\n        D12 & D22 & E2A & E2B & E3A & E3B --> F2[Get Resolution Status]\n        F2 --> G2{Determine Status}\n        G2 -->|FULL| H2[Resolved Full]\n        G2 -->|PARTIAL| I2[Resolved Partial]\n        G2 -->|NO| J2[Resolved No]\n        H2 & I2 & J2 --> K2[End]\n    end\n\n    subgraph monitor [\"monitor.py\"]\n        A3[Start monitor_validation] --> B3[Iterate through log files]\n        B3 --> C3{Check if patch applied}\n        C3 -->|Yes| D3[Count patch applies]\n        D3 --> E3{Check for TESTS_TIMEOUT}\n        E3 -->|Yes| F3[Append to timeout list]\n        E3 -->|No| G3{Check patch applies}\n        G3 -->|0| H3[Append to corrupt_test_patch list]\n        G3 -->|1| I3[Append to corrupt_patch list]\n        G3 -->|2| J3[Append to success list]\n        C3 -->|No| K3[Append to failed_install list]\n        L3[End monitor_validation] <-- F3\n        L3 <-- H3\n        L3 <-- I3\n        L3 <-- J3\n        L3 <-- K3\n        L3 --> M3[Log results]\n        M3 --> N3[Assert all instances accounted for]\n        N3 --> O3[Return lists]\n        P3[Start monitor_logs_same_diff] --> Q3[Iterate through log files]\n        Q3 --> R3{Check if repo provided}\n        R3 -->|Yes| S3[Use provided repo]\n        R3 -->|No| T3[Get repo from log path]\n        S3 --> U3[Get log parser]\n        T3 --> U3\n        U3 --> V3[Get pre, post patch behavior]\n        V3 --> W3{Check if behavior is same}\n        W3 -->|Yes| X3[Append to logs_same list]\n        W3 -->|No| Y3[Append to logs_diff list]\n        Z3[End monitor_logs_same_diff] <-- X3\n        Z3 <-- Y3\n        Z3 --> AA3[Return lists]\n    end\n\n    subgraph report [\"report.py\"]\n        A4[Start] --> B4{Load Evaluation Logs}\n        B4 --> C4{Load Gold Results}\n        C4 --> D4{Calculate Metrics}\n        D4 --> E4{Generate Report}\n        E4 --> F4{Generate Summary}\n        F4 --> G4[End]\n        subgraph get_eval_report\n            D4 -->|Fail to Pass| F2P[Calculate F2P Metrics]\n            D4 -->|Pass to Pass| P2P[Calculate P2P Metrics]\n            D4 -->|Fail to Fail| F2F[Calculate F2F Metrics]\n            D4 -->|Pass to Fail| P2F[Calculate P2F Metrics]\n        end\n        subgraph get_eval_reports_for_logs\n            B4 -->|For Each Log| C4\n            C4 -->|Find Corresponding Gold Result| D4\n            D4 -->|For Each Test Case| E4\n        end\n        subgraph get_eval_reports_for_dir\n            A4 -->|Load Logs from Directory| B4\n        end\n        subgraph get_model_eval_summary\n            A4 -->|Load Predictions| preds[Load Predictions]\n            preds -->|Filter by Repo| filter[Filter Predictions]\n            filter -->|Get Reports| get_eval_reports_for_dir\n            get_eval_reports_for_dir -->|Compile Summary| F4\n        end\n        subgraph get_model_report\n            A4 -->|Load Predictions| predictions[Load Predictions]\n            predictions -->|For Each Prediction| check[Check Model Patch]\n            check -->|Exists| exists[Model Patch Exists]\n            exists -->|Log Exists| log_exists[Log File Exists]\n            log_exists -->|Eval Log Found| found[Eval Log Found]\n            found -->|Generate Report| get_eval_report\n            get_eval_report -->|Resolution Status| resolution[Check Resolution Status]\n            resolution -->|Compile Report| G4\n        end\n    end\n\n    conversion --> getters\n    getters --> log_parsers\n    log_parsers --> metrics\n    metrics --> monitor\n    monitor --> report\n```\nThis mermaid diagram illustrates the interconnected processes and functionalities of the files within the `.docy/docs/json/metrics` folder of the project. It starts with the conversion of log files to ground truth, progresses through getting log file paths, parsing logs based on the testing framework, computing metrics, monitoring validation and log differences, and finally generating reports. Each step is crucial for the workflow, contributing to the project's goal of analyzing and reporting on software evaluation metrics.","metadata":{"source":".docy/docs/markdown/metrics/summary.md"}}],["46",{"pageContent":"```mermaid\nflowchart TB\n    subgraph swe_bench_tutorials\n    direction TB\n    A[Start] --> B{Import Modules}\n    B --> C[Declare Repository and Log Directory]\n    C --> D[Load and Sort Task Instances]\n    D --> E[Monitor Validation]\n    E --> F[Monitor Logs for Same/Different Outputs]\n    F --> G[Convert Logs to Ground Truth for Different Outputs]\n    G --> H[Filter and Prepare Final Task Instances]\n    H --> I[Save Final Task Instances to JSON]\n    I --> J[End]\n    end\n\n    subgraph other_parts_of_project\n    direction TB\n    K[Other Project Modules] --> L{Data Collection}\n    L --> M[Data Preprocessing]\n    M --> N[Model Training]\n    N --> O[Model Evaluation]\n    O --> P[Deployment]\n    end\n\n    swe_bench_tutorials --> other_parts_of_project\n\n    style swe_bench_tutorials fill:#bbf,stroke:#333,stroke-width:2px\n    style other_parts_of_project fill:#bfb,stroke:#333,stroke-width:2px\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style J fill:#f9f,stroke:#333,stroke-width:4px\n```\n\nThis mermaid markdown code illustrates the steps of the code within the `swe-bench` project's `tutorials` folder, specifically focusing on the validation process. It also shows how this process might fit into the larger project by connecting the tutorial's end to other project modules, indicating a flow from data collection to deployment. This visualization helps in understanding the role of the tutorial within the context of the entire project, emphasizing its position in preparing and validating task instances before they are utilized in further project stages like model training and evaluation.","metadata":{"source":".docy/docs/markdown/tutorials/summary.md"}}],["47",{"pageContent":"```mermaid\nflowchart TD\n    A[Start] --> B{Import Modules}\n    B --> C[Declare Repository and Log Directory]\n    C --> D[Load and Sort Task Instances]\n    D --> E[Monitor Validation]\n    E --> F[Monitor Logs for Same/Different Outputs]\n    F --> G[Convert Logs to Ground Truth for Different Outputs]\n    G --> H[Filter and Prepare Final Task Instances]\n    H --> I[Save Final Task Instances to JSON]\n    I --> J[End]\n    \n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style J fill:#f9f,stroke:#333,stroke-width:4px\n```\nThis flowchart represents the sequential steps taken by the code in the provided file. Starting with importing necessary modules, it moves through declaring the repository and log directory, loading and sorting task instances, monitoring validation and log outputs, converting logs to ground truth for differing outputs, filtering and preparing final task instances, and finally saving these instances to a JSON file.","metadata":{"source":".docy/docs/markdown/tutorials/validation.md"}}],["48",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{For each repository}\n    B --> C[Identify version file paths]\n    B --> D[Identify version regex patterns]\n    C --> E[Map repo to version file paths]\n    D --> F[Map repo to version regex patterns]\n    E --> G[Prepare version file URLs]\n    F --> H[Prepare regex for version extraction]\n    G --> I[Combine URLs with SWE_BENCH_URL_RAW]\n    H --> J[Define version extraction methods]\n    I --> K[Ready for version check]\n    J --> K\n    K --> L[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style L fill:#f9f,stroke:#333,stroke-width:4px\n```\nThis diagram illustrates the process of mapping repositories to their version file paths and regex patterns for version extraction within the `swe-bench` project. It starts by identifying the necessary file paths and regex patterns for each repository, then maps these to prepare URLs and regex methods for version checking, and finally combines these elements to ready the system for a version check operation.","metadata":{"source":".docy/docs/markdown/versioning/constants.md"}}],["49",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Import Dependencies]\n\n    B --> C[Get Raw Astropy Dataset]\n    C --> D[Get Version to Date from Astropy Homepage]\n\n    D --> E[Extract (Date, Version) Pairs]\n    E --> F[Construct (Version, Date) Pairs]\n    F --> G[Group Times by Major/Minor Version]\n    G --> H[Pick Most Recent Time as Version Cut Off Date]\n\n    H --> I[Assign Version to Each Task Instance]\n    I --> J[Construct Map of Versions to Task Instances]\n\n    J --> K[Save Matplotlib Versioned Data to Repository]\n    K --> L[End]\n```","metadata":{"source":".docy/docs/markdown/versioning/extract_web/get_versions_astropy.md"}}],["50",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Import Libraries]\n    B --> C[Load Matplotlib Task Instances]\n    C --> D[Fetch Matplotlib Version History]\n    D --> E[Extract Version and Date Information]\n    E --> F[Filter and Format Dates and Versions]\n    F --> G[Assign Versions to Tasks Based on Date]\n    G --> H[Map Versions to Task Instances]\n    H --> I[Save Versioned Data to File]\n    I --> J[End]\n```","metadata":{"source":".docy/docs/markdown/versioning/extract_web/get_versions_matplotlib.md"}}],["51",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B(Get raw dataset)\n    B --> C(Get version to date from webpage)\n    C --> D{Iterate through matches}\n    D --> E[Construct (version, date) pairs]\n    E --> F[Group times by major/minor version]\n    F --> G[Pick most recent time as version cut-off date]\n    G --> H{Assign version to each task instance}\n    H --> I[Construct map of versions to task instances]\n    I --> J[Save versioned data to repository]\n    J --> K[End]\n```","metadata":{"source":".docy/docs/markdown/versioning/extract_web/get_versions_pvlib-python.md"}}],["52",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Get pydicom task instances}\n    B --> C[Fetch pydicom FAQ page]\n    C --> D[Parse HTML for release table]\n    D --> E[Extract release dates and versions]\n    E --> F[Sort releases by date]\n    F --> G{For each task instance}\n    G -->|For each release| H[Check if release date < task creation date]\n    H -->|True| I[Assign version to task]\n    H -->|False| J[Assign oldest version to task]\n    I --> K[Continue to next task]\n    J --> K\n    K --> L{All tasks processed}\n    L -->|True| M[Write updated tasks to file]\n    M --> N[End]\n```\nThis flowchart represents the process of updating pydicom task instances with their corresponding pydicom version based on the task's creation date and the release dates of pydicom versions. It starts by fetching the task instances and the pydicom FAQ page, then parses the release information, and finally updates each task instance with the appropriate version before saving the updated list.","metadata":{"source":".docy/docs/markdown/versioning/extract_web/get_versions_pydicom.md"}}],["53",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Get raw sqlfluff dataset}\n    B --> C[Get all GitHub releases]\n    C --> D[Extract version and date from releases]\n    D --> E[Collect version/date pairs]\n    E --> F[Sort version/date pairs]\n    F --> G[Iterate through data_tasks]\n    G --> H[Assign versions to tasks]\n    H --> I[Save versioned data to repository]\n    I --> J[Print all versions]\n    J --> K[End]\n    \n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style K fill:#f9f,stroke:#333,stroke-width:4px\n```","metadata":{"source":".docy/docs/markdown/versioning/extract_web/get_versions_sqlfluff.md"}}],["54",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Import Libraries]\n\n    B --> C[Append harness utils to sys.path]\n    C --> D[Get raw xarray dataset instances]\n\n    D --> E[Fetch xarray version info from webpage]\n    E --> F[Extract version and date info using regex]\n    F --> G[Format and sort version-date pairs]\n\n    G --> H[Assign version to each task based on creation date]\n    H --> I[Save versioned xarray task instances to file]\n\n    I --> J[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style J fill:#f9f,stroke:#333,stroke-width:4px\n```\nThis flowchart represents the sequence of operations performed by the code snippet. It starts with importing necessary libraries and ends with saving the versioned xarray task instances to a file. The process involves fetching and processing version information from the xarray documentation webpage, then assigning the appropriate version to each task based on its creation date before finally saving this enriched dataset.","metadata":{"source":".docy/docs/markdown/versioning/extract_web/get_versions_xarray.md"}}],["55",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B[Import Necessary Libraries/Dependencies]\n    B --> C[Fetch Task Instances]\n    C --> D[Fetch Version Information]\n    D --> E[Extract Version and Date Information]\n    E --> F[Format and Sort Version-Date Pairs]\n    F --> G[Assign Versions to Tasks Based on Date]\n    G --> H[Map Versions to Task Instances]\n    H --> I[Save Versioned Data]\n    I --> J[End]\n\n    subgraph astropy\n        D1[Get Raw Astropy Dataset] --> D2[Get Version to Date from Astropy Homepage] --> E1[Extract (Date, Version) Pairs]\n    end\n\n    subgraph matplotlib\n        D3[Load Matplotlib Task Instances] --> D4[Fetch Matplotlib Version History] --> E2[Extract Version and Date Information]\n    end\n\n    subgraph pvlib-python\n        D5[Get raw dataset from webpage] --> D6[Iterate through matches] --> E3[Construct (version, date) pairs]\n    end\n\n    subgraph pydicom\n        D7[Fetch pydicom FAQ page] --> D8[Parse HTML for release table] --> E4[Extract release dates and versions]\n    end\n\n    subgraph sqlfluff\n        D9[Get all GitHub releases] --> D10[Extract version and date from releases] --> E5[Collect version/date pairs]\n    end\n\n    subgraph xarray\n        D11[Fetch xarray version info from webpage] --> D12[Extract version and date info using regex] --> E6[Format and sort version-date pairs]\n    end\n\n    style astropy fill:#bbf,stroke:#333,stroke-width:2px\n    style matplotlib fill:#bbf,stroke:#333,stroke-width:2px\n    style pvlib-python fill:#bbf,stroke:#333,stroke-width:2px\n    style pydicom fill:#bbf,stroke:#333,stroke-width:2px\n    style sqlfluff fill:#bbf,stroke:#333,stroke-width:2px\n    style xarray fill:#bbf,stroke:#333,stroke-width:2px\n```\n\nThis mermaid markdown code illustrates the general workflow for extracting and processing version information for various libraries (Astropy, Matplotlib, pvlib-python, pydicom, sqlfluff, and xarray) within the swe-bench project. Each library follows a similar pattern of importing necessary libraries or dependencies, fetching task instances, fetching version information from different sources (homepage, GitHub releases, FAQ pages, etc.), extracting and formatting version-date pairs, assigning versions to tasks based on dates, mapping these versions to task instances, and finally saving the versioned data. The subgraphs represent the specific processes for each library, highlighting the unique steps taken to extract and process version information for that library.","metadata":{"source":".docy/docs/markdown/versioning/extract_web/summary.md"}}],["56",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Parse Arguments}\n    B --> C[Get Task Instances]\n    C --> D{Split Instances for Threads}\n    D --> E{Determine Retrieval Method}\n    E -->|GitHub| F[Get Versions from Web]\n    E -->|Build| G[Get Versions from Build]\n    E -->|Mix| H[Get Versions from Web and Build]\n    F --> I[Merge Results and Cleanup]\n    G --> I\n    H --> I\n    I --> J[End]\n\n    subgraph web \"Web Version Retrieval\"\n        F --> F1[Search GitHub for Versions]\n        F1 --> F2[Save Versions to File]\n    end\n\n    subgraph build \"Build Version Retrieval\"\n        G --> G1[Setup Environment]\n        G1 --> G2[Clone Repo and Conda Env per Thread]\n        G2 --> G3[Build Repo and Retrieve Versions]\n        G3 --> G4[Save Versions to File]\n    end\n\n    subgraph mix \"Mixed Version Retrieval\"\n        H --> H1[Search GitHub for Versions]\n        H1 --> H2[Filter Instances Not Found]\n        H2 --> H3[Setup Environment]\n        H3 --> H4[Clone Repo and Conda Env per Thread]\n        H4 --> H5[Build Repo and Retrieve Versions]\n        H5 --> H6[Save Versions to File]\n    end\n```","metadata":{"source":".docy/docs/markdown/versioning/get_versions.md"}}],["57",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Determine Retrieval Method}\n\n    B -->|build| C[Setup Conda Environment]\n    B -->|github| D[Setup GitHub Retrieval]\n\n    C --> E[Build Task Instances Locally]\n    D --> F[Retrieve Task Instances from GitHub]\n\n    E --> G[Specify Number of Threads for Building]\n    F --> H[Specify Number of Workers for Retrieval]\n\n    G --> I[Define Path to Conda Installation]\n    H --> J[Define Output Directory for Saved Instances]\n\n    I --> K[Execute Local Building Process]\n    J --> L[Execute GitHub Retrieval Process]\n\n    K --> M[End]\n    L --> M\n```\nThis mermaid diagram illustrates the steps involved in the `get_versions.py` script from the `swe-bench` project, focusing on the process of retrieving versions of task instances either by building them locally or fetching them from GitHub, based on the specified retrieval method.","metadata":{"source":".docy/docs/markdown/versioning/run_get_versions.md"}}],["58",{"pageContent":"```mermaid\ngraph TD\n    A[Start Versioning Process] --> B[Constants Setup]\n    B --> C[Get Versions]\n    B --> D[Run Get Versions Script]\n    B --> E[Utilize Utilities]\n    B --> F[Extract Web Version Information]\n\n    C --> G[Merge and Cleanup Version Data]\n    D --> H[Local Build or GitHub Retrieval]\n    E --> I[File Type Check and Instance Splitting]\n    F --> J[Version and Date Extraction for Libraries]\n\n    G --> K[Version Data Ready for Analysis]\n    H --> K\n    I --> K\n    J --> K\n\n    K --> L[Map Versions to Task Instances]\n    L --> M[Save Versioned Data]\n    M --> N[End Versioning Process]\n\n    subgraph Constants\n        B --> B1[Identify Version File Paths]\n        B --> B2[Identify Version Regex Patterns]\n        B1 --> B3[Prepare Version File URLs]\n        B2 --> B4[Prepare Regex for Version Extraction]\n        B3 --> B5[Combine URLs with SWE_BENCH_URL_RAW]\n        B4 --> B6[Define Version Extraction Methods]\n    end\n\n    subgraph Get Versions\n        C --> C1[Parse Arguments]\n        C1 --> C2[Get Task Instances]\n        C2 --> C3[Split Instances for Threads]\n        C3 --> C4[Determine Retrieval Method]\n        C4 --> C5[Version Retrieval]\n        C5 --> G\n    end\n\n    subgraph Run Get Versions Script\n        D --> D1[Determine Retrieval Method]\n        D1 --> D2[Setup Environment]\n        D2 --> D3[Specify Threads/Workers]\n        D3 --> D4[Define Paths and Directories]\n        D4 --> D5[Execute Retrieval Process]\n        D5 --> H\n    end\n\n    subgraph Utilize Utilities\n        E --> E1[Check File Type]\n        E1 --> E2[Read and Load JSON]\n        E2 --> E3[Split Instances]\n        E3 --> I\n    end\n\n    subgraph Extract Web Version Information\n        F --> F1[Import Libraries/Dependencies]\n        F1 --> F2[Fetch Task Instances]\n        F2 --> F3[Fetch and Extract Version Information]\n        F3 --> F4[Format and Sort Version-Date Pairs]\n        F4 --> F5[Assign Versions Based on Date]\n        F5 --> J\n    end\n\n    style Constants fill:#f9f,stroke:#333,stroke-width:2px\n    style Get Versions fill:#bbf,stroke:#333,stroke-width:2px\n    style Run Get Versions Script fill:#bbf,stroke:#333,stroke-width:2px\n    style Utilize Utilities fill:#bbf,stroke:#333,stroke-width:2px\n    style Extract Web Version Information fill:#bbf,stroke:#333,stroke-width:2px\n```","metadata":{"source":".docy/docs/markdown/versioning/summary.md"}}],["59",{"pageContent":"```mermaid\ngraph TD\n    A[Start] --> B{Check File Type}\n    B -->|JSONL or JSONL.all| C[Read Line by Line]\n    B -->|Other JSON| D[Read Whole File]\n    C --> E[Load JSON Objects]\n    D --> E\n    E --> F[get_instances: Return Task Instances]\n    F --> G{split_instances}\n    G -->|Split List| H[Calculate Average Length]\n    H --> I[Calculate Remainder]\n    I --> J[Split into Sublists]\n    J --> K[Return List of Sublists]\n```\nThis flowchart represents the process described in the provided code. It starts with determining the type of file based on its extension, then proceeds to read and load the task instances from the file. After obtaining the task instances, it demonstrates how the list of instances can be split into approximately equal-length sublists.","metadata":{"source":".docy/docs/markdown/versioning/utils.md"}}]]