{
  "folderName": "metrics",
  "folderPath": ".docy/docs/json/metrics",
  "url": "/tree/master/.docy/docs/json/metrics",
  "files": [
    {
      "fileName": "conversion.py",
      "filePath": "metrics/conversion.py",
      "url": "/blob/master/metrics/conversion.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start convert_log_to_ground_truth] --> B{Check if log file can be parsed}\n    B -- Yes --> C[Retrieve Instance File Name and Repository]\n    B -- No --> Z[Error: Log file could not be parsed]\n    C --> D[Identify Log Parser based on Repository]\n    D --> E[Extract Before and After Test Status]\n    E --> F{Loop through After Test Status}\n    F -->|Test Passed| G[Test Passed in Before Status]\n    F -->|Test Failed| H[Test Failed in Before Status]\n    G --> I{Was Test Initially Passing?}\n    H --> J{Was Test Initially Failing?}\n    I -- Yes --> K[Add to PASS_TO_PASS]\n    I -- No --> L[Add to FAIL_TO_PASS]\n    J -- Yes --> M[Add to PASS_TO_FAIL]\n    J -- No --> N[Add to FAIL_TO_FAIL]\n    K & L & M & N --> O{Is Save Directory Provided?}\n    O -- Yes --> P[Save Results to File]\n    O -- No --> Q[Return Status Ground Truth]\n    P --> Q[Return Status Ground Truth]\n    Z --> Q[Return Status Ground Truth]\n```\nThis mermaid markdown code illustrates the steps involved in the `convert_log_to_ground_truth` function from the provided code snippet. It starts with checking if the log file can be parsed, then moves through identifying the repository, selecting the appropriate log parser, extracting before and after test statuses, categorizing tests based on their transitions between statuses, and optionally saving the results to a file if a save directory is provided.",
      "questions": "",
      "checksum": "2edfdf9fdeeedcddc4a09a49f3cad28d"
    },
    {
      "fileName": "getters.py",
      "filePath": "metrics/getters.py",
      "url": "/blob/master/metrics/getters.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Get Log File Path}\n    B --> C[Identify Repository]\n    C --> D{Check Log Type}\n    D -->|Evaluation Log| E[Check Patch Application Success]\n    D -->|Validation Log| F[Split Pre and Post Patch Logs]\n    E -->|Success| G[Parse Evaluation Results]\n    E -->|Fail| H[Return Empty Status Map, False]\n    F -->|Found 2 Patches| I[Parse Pre and Post Patch Logs]\n    F -->|Less than 2 Patches| J[Return None, None]\n    G --> K[Return Status Map, True]\n    I --> L{Attempt Parsing Logs}\n    L -->|Success| M[Return Status Maps List, True]\n    L -->|Fail| N[Return None, False]\n    M --> O[End]\n    N --> O\n    K --> O\n    J --> O\n    H --> O\n```\nThis flowchart represents the process of handling log files in the `swe-bench` project. It starts with obtaining the log file path and identifying the repository associated with it. Based on the log type, it either checks for patch application success in evaluation logs or splits the log into pre and post-patch logs for validation logs. For evaluation logs, if the patch application is successful, it parses the evaluation results; otherwise, it returns an empty status map and `False`. For validation logs, if two patches are found, it attempts to parse the pre and post-patch logs; otherwise, it returns `None` for both. If parsing is successful, it returns a list of status maps and `True`; if not, it returns `None` and `False`. The process ends after handling the logs accordingly.",
      "questions": "",
      "checksum": "049eae111ce60541af77c9eaa8fd2f0c"
    },
    {
      "fileName": "log_parsers.py",
      "filePath": "metrics/log_parsers.py",
      "url": "/blob/master/metrics/log_parsers.py",
      "summary": "```mermaid\nflowchart TB\n    start[Start] --> selectFramework{Select Testing Framework}\n    selectFramework --> pytest[PyTest]\n    selectFramework --> django[Django Tester]\n    selectFramework --> pytest_v2[PyTest v2]\n    selectFramework --> seaborn[Seaborn]\n    selectFramework --> sympy[Sympy]\n\n    pytest --> parse_pytest[Parse PyTest Log]\n    django --> parse_django[Parse Django Log]\n    pytest_v2 --> parse_pytest_v2[Parse PyTest v2 Log]\n    seaborn --> parse_seaborn[Parse Seaborn Log]\n    sympy --> parse_sympy[Parse Sympy Log]\n\n    parse_pytest --> mapStatusPytest{Map Status}\n    parse_django --> mapStatusDjango{Map Status}\n    parse_pytest_v2 --> mapStatusPytest_v2{Map Status}\n    parse_seaborn --> mapStatusSeaborn{Map Status}\n    parse_sympy --> mapStatusSympy{Map Status}\n\n    mapStatusPytest --> end[End]\n    mapStatusDjango --> end\n    mapStatusPytest_v2 --> end\n    mapStatusSeaborn --> end\n    mapStatusSympy --> end\n\n    classDef framework fill:#f9f,stroke:#333,stroke-width:2px;\n    classDef action fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef mapping fill:#fb4,stroke:#333,stroke-width:2px;\n    class selectFramework framework;\n    class parse_pytest,parse_django,parse_pytest_v2,parse_seaborn,parse_sympy action;\n    class mapStatusPytest,mapStatusDjango,mapStatusPytest_v2,mapStatusSeaborn,mapStatusSympy mapping;\n```\nThis flowchart represents the process of selecting a testing framework and parsing its log to map test cases to their statuses. Each testing framework (PyTest, Django Tester, PyTest v2, Seaborn, and Sympy) has a specific parsing function that processes the log and maps the status of each test case.",
      "questions": "",
      "checksum": "4106daba4b60cecf2b2de552f3e06d2b"
    },
    {
      "fileName": "metrics.py",
      "filePath": "metrics/metrics.py",
      "url": "/blob/master/metrics/metrics.py",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Compute Metrics]\n    B --> C{Is it a single report?}\n    C -->|Yes| D[Compute Single Report Metrics]\n    C -->|No| E[Compute Multiple Reports Metrics]\n    D --> D1[Compute Fail-to-Pass for Single Report]\n    D --> D2[Compute Pass-to-Pass for Single Report]\n    E --> E1{Is it weighted?}\n    E1 -->|Yes| E2[Compute Weighted Metrics]\n    E1 -->|No| E3[Compute Unweighted Metrics]\n    E2 --> E2A[Compute Weighted Fail-to-Pass]\n    E2 --> E2B[Compute Weighted Pass-to-Pass]\n    E3 --> E3A[Compute Unweighted Fail-to-Pass]\n    E3 --> E3B[Compute Unweighted Pass-to-Pass]\n    D1 & D2 & E2A & E2B & E3A & E3B --> F[Get Resolution Status]\n    F --> G{Determine Status}\n    G -->|FULL| H[Resolved Full]\n    G -->|PARTIAL| I[Resolved Partial]\n    G -->|NO| J[Resolved No]\n    H & I & J --> K[End]\n```\nThis mermaid diagram illustrates the flow of operations within the provided code. It starts with the decision of whether the input is a single report or multiple reports, followed by further decisions based on the type of computation (weighted or unweighted for multiple reports) and concludes with determining the resolution status based on the computed metrics.",
      "questions": "",
      "checksum": "dcc82eae5835805db49754cbf53da494"
    },
    {
      "fileName": "monitor.py",
      "filePath": "metrics/monitor.py",
      "url": "/blob/master/metrics/monitor.py",
      "summary": "```mermaid\ngraph TD\n    A[Start monitor_validation] --> B[Iterate through log files]\n    B --> C{Check if patch applied}\n    C -->|Yes| D[Count patch applies]\n    D --> E{Check for TESTS_TIMEOUT}\n    E -->|Yes| F[Append to timeout list]\n    E -->|No| G{Check patch applies}\n    G -->|0| H[Append to corrupt_test_patch list]\n    G -->|1| I[Append to corrupt_patch list]\n    G -->|2| J[Append to success list]\n    C -->|No| K[Append to failed_install list]\n    L[End monitor_validation] <-- F\n    L <-- H\n    L <-- I\n    L <-- J\n    L <-- K\n    L --> M[Log results]\n    M --> N[Assert all instances accounted for]\n    N --> O[Return lists]\n\n    P[Start monitor_logs_same_diff] --> Q[Iterate through log files]\n    Q --> R{Check if repo provided}\n    R -->|Yes| S[Use provided repo]\n    R -->|No| T[Get repo from log path]\n    S --> U[Get log parser]\n    T --> U\n    U --> V[Get pre, post patch behavior]\n    V --> W{Check if behavior is same}\n    W -->|Yes| X[Append to logs_same list]\n    W -->|No| Y[Append to logs_diff list]\n    Z[End monitor_logs_same_diff] <-- X\n    Z <-- Y\n    Z --> AA[Return lists]\n```\nThis mermaid diagram illustrates the flow and decision-making process within the `monitor_validation` and `monitor_logs_same_diff` functions. It shows how log files are processed, analyzed for specific conditions (such as patch application success, timeouts, and pre/post patch behavior differences), and categorized accordingly.",
      "questions": "",
      "checksum": "477807fc92b3b0658b06cf2d60b1e29b"
    },
    {
      "fileName": "report.py",
      "filePath": "metrics/report.py",
      "url": "/blob/master/metrics/report.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Load Evaluation Logs}\n    B --> C{Load Gold Results}\n    C --> D{Calculate Metrics}\n    D --> E{Generate Report}\n    E --> F{Generate Summary}\n    F --> G[End]\n\n    subgraph get_eval_report\n        D -->|Fail to Pass| F2P[Calculate F2P Metrics]\n        D -->|Pass to Pass| P2P[Calculate P2P Metrics]\n        D -->|Fail to Fail| F2F[Calculate F2F Metrics]\n        D -->|Pass to Fail| P2F[Calculate P2F Metrics]\n    end\n\n    subgraph get_eval_reports_for_logs\n        B -->|For Each Log| C\n        C -->|Find Corresponding Gold Result| D\n        D -->|For Each Test Case| E\n    end\n\n    subgraph get_eval_reports_for_dir\n        A -->|Load Logs from Directory| B\n    end\n\n    subgraph get_model_eval_summary\n        A -->|Load Predictions| preds[Load Predictions]\n        preds -->|Filter by Repo| filter[Filter Predictions]\n        filter -->|Get Reports| get_eval_reports_for_dir\n        get_eval_reports_for_dir -->|Compile Summary| F\n    end\n\n    subgraph get_model_report\n        A -->|Load Predictions| predictions[Load Predictions]\n        predictions -->|For Each Prediction| check[Check Model Patch]\n        check -->|Exists| exists[Model Patch Exists]\n        exists -->|Log Exists| log_exists[Log File Exists]\n        log_exists -->|Eval Log Found| found[Eval Log Found]\n        found -->|Generate Report| get_eval_report\n        get_eval_report -->|Resolution Status| resolution[Check Resolution Status]\n        resolution -->|Compile Report| G\n    end\n```\nThis mermaid diagram illustrates the flow and functionalities of the provided code snippets, focusing on the evaluation report generation, model evaluation summary, and model report generation processes within the larger project context.",
      "questions": "",
      "checksum": "be46ea96abee610f82989e3b51ccfde4"
    }
  ],
  "folders": [],
  "summary": "```mermaid\nflowchart TB\n    subgraph conversion [\"conversion.py\"]\n        A[Start convert_log_to_ground_truth] --> B{Check if log file can be parsed}\n        B -- Yes --> C[Retrieve Instance File Name and Repository]\n        B -- No --> Z[Error: Log file could not be parsed]\n        C --> D[Identify Log Parser based on Repository]\n        D --> E[Extract Before and After Test Status]\n        E --> F{Loop through After Test Status}\n        F -->|Test Passed| G[Test Passed in Before Status]\n        F -->|Test Failed| H[Test Failed in Before Status]\n        G --> I{Was Test Initially Passing?}\n        H --> J{Was Test Initially Failing?}\n        I -- Yes --> K[Add to PASS_TO_PASS]\n        I -- No --> L[Add to FAIL_TO_PASS]\n        J -- Yes --> M[Add to PASS_TO_FAIL]\n        J -- No --> N[Add to FAIL_TO_FAIL]\n        K & L & M & N --> O{Is Save Directory Provided?}\n        O -- Yes --> P[Save Results to File]\n        O -- No --> Q[Return Status Ground Truth]\n        P --> Q[Return Status Ground Truth]\n        Z --> Q[Return Status Ground Truth]\n    end\n\n    subgraph getters [\"getters.py\"]\n        A1[Start] --> B1{Get Log File Path}\n        B1 --> C1[Identify Repository]\n        C1 --> D1{Check Log Type}\n        D1 -->|Evaluation Log| E1[Check Patch Application Success]\n        D1 -->|Validation Log| F1[Split Pre and Post Patch Logs]\n        E1 -->|Success| G1[Parse Evaluation Results]\n        E1 -->|Fail| H1[Return Empty Status Map, False]\n        F1 -->|Found 2 Patches| I1[Parse Pre and Post Patch Logs]\n        F1 -->|Less than 2 Patches| J1[Return None, None]\n        G1 --> K1[Return Status Map, True]\n        I1 --> L1{Attempt Parsing Logs}\n        L1 -->|Success| M1[Return Status Maps List, True]\n        L1 -->|Fail| N1[Return None, False]\n        M1 --> O1[End]\n        N1 --> O1\n        K1 --> O1\n        J1 --> O1\n        H1 --> O1\n    end\n\n    subgraph log_parsers [\"log_parsers.py\"]\n        start1[Start] --> selectFramework{Select Testing Framework}\n        selectFramework --> pytest[PyTest]\n        selectFramework --> django[Django Tester]\n        selectFramework --> pytest_v2[PyTest v2]\n        selectFramework --> seaborn[Seaborn]\n        selectFramework --> sympy[Sympy]\n        pytest --> parse_pytest[Parse PyTest Log]\n        django --> parse_django[Parse Django Log]\n        pytest_v2 --> parse_pytest_v2[Parse PyTest v2 Log]\n        seaborn --> parse_seaborn[Parse Seaborn Log]\n        sympy --> parse_sympy[Parse Sympy Log]\n        parse_pytest --> mapStatusPytest{Map Status}\n        parse_django --> mapStatusDjango{Map Status}\n        parse_pytest_v2 --> mapStatusPytest_v2{Map Status}\n        parse_seaborn --> mapStatusSeaborn{Map Status}\n        parse_sympy --> mapStatusSympy{Map Status}\n        mapStatusPytest --> end1[End]\n        mapStatusDjango --> end1\n        mapStatusPytest_v2 --> end1\n        mapStatusSeaborn --> end1\n        mapStatusSympy --> end1\n    end\n\n    subgraph metrics [\"metrics.py\"]\n        A2[Start] --> B2[Compute Metrics]\n        B2 --> C2{Is it a single report?}\n        C2 -->|Yes| D2[Compute Single Report Metrics]\n        C2 -->|No| E2[Compute Multiple Reports Metrics]\n        D2 --> D12[Compute Fail-to-Pass for Single Report]\n        D2 --> D22[Compute Pass-to-Pass for Single Report]\n        E2 --> E12{Is it weighted?}\n        E12 -->|Yes| E22[Compute Weighted Metrics]\n        E12 -->|No| E32[Compute Unweighted Metrics]\n        E22 --> E2A[Compute Weighted Fail-to-Pass]\n        E22 --> E2B[Compute Weighted Pass-to-Pass]\n        E32 --> E3A[Compute Unweighted Fail-to-Pass]\n        E32 --> E3B[Compute Unweighted Pass-to-Pass]\n        D12 & D22 & E2A & E2B & E3A & E3B --> F2[Get Resolution Status]\n        F2 --> G2{Determine Status}\n        G2 -->|FULL| H2[Resolved Full]\n        G2 -->|PARTIAL| I2[Resolved Partial]\n        G2 -->|NO| J2[Resolved No]\n        H2 & I2 & J2 --> K2[End]\n    end\n\n    subgraph monitor [\"monitor.py\"]\n        A3[Start monitor_validation] --> B3[Iterate through log files]\n        B3 --> C3{Check if patch applied}\n        C3 -->|Yes| D3[Count patch applies]\n        D3 --> E3{Check for TESTS_TIMEOUT}\n        E3 -->|Yes| F3[Append to timeout list]\n        E3 -->|No| G3{Check patch applies}\n        G3 -->|0| H3[Append to corrupt_test_patch list]\n        G3 -->|1| I3[Append to corrupt_patch list]\n        G3 -->|2| J3[Append to success list]\n        C3 -->|No| K3[Append to failed_install list]\n        L3[End monitor_validation] <-- F3\n        L3 <-- H3\n        L3 <-- I3\n        L3 <-- J3\n        L3 <-- K3\n        L3 --> M3[Log results]\n        M3 --> N3[Assert all instances accounted for]\n        N3 --> O3[Return lists]\n        P3[Start monitor_logs_same_diff] --> Q3[Iterate through log files]\n        Q3 --> R3{Check if repo provided}\n        R3 -->|Yes| S3[Use provided repo]\n        R3 -->|No| T3[Get repo from log path]\n        S3 --> U3[Get log parser]\n        T3 --> U3\n        U3 --> V3[Get pre, post patch behavior]\n        V3 --> W3{Check if behavior is same}\n        W3 -->|Yes| X3[Append to logs_same list]\n        W3 -->|No| Y3[Append to logs_diff list]\n        Z3[End monitor_logs_same_diff] <-- X3\n        Z3 <-- Y3\n        Z3 --> AA3[Return lists]\n    end\n\n    subgraph report [\"report.py\"]\n        A4[Start] --> B4{Load Evaluation Logs}\n        B4 --> C4{Load Gold Results}\n        C4 --> D4{Calculate Metrics}\n        D4 --> E4{Generate Report}\n        E4 --> F4{Generate Summary}\n        F4 --> G4[End]\n        subgraph get_eval_report\n            D4 -->|Fail to Pass| F2P[Calculate F2P Metrics]\n            D4 -->|Pass to Pass| P2P[Calculate P2P Metrics]\n            D4 -->|Fail to Fail| F2F[Calculate F2F Metrics]\n            D4 -->|Pass to Fail| P2F[Calculate P2F Metrics]\n        end\n        subgraph get_eval_reports_for_logs\n            B4 -->|For Each Log| C4\n            C4 -->|Find Corresponding Gold Result| D4\n            D4 -->|For Each Test Case| E4\n        end\n        subgraph get_eval_reports_for_dir\n            A4 -->|Load Logs from Directory| B4\n        end\n        subgraph get_model_eval_summary\n            A4 -->|Load Predictions| preds[Load Predictions]\n            preds -->|Filter by Repo| filter[Filter Predictions]\n            filter -->|Get Reports| get_eval_reports_for_dir\n            get_eval_reports_for_dir -->|Compile Summary| F4\n        end\n        subgraph get_model_report\n            A4 -->|Load Predictions| predictions[Load Predictions]\n            predictions -->|For Each Prediction| check[Check Model Patch]\n            check -->|Exists| exists[Model Patch Exists]\n            exists -->|Log Exists| log_exists[Log File Exists]\n            log_exists -->|Eval Log Found| found[Eval Log Found]\n            found -->|Generate Report| get_eval_report\n            get_eval_report -->|Resolution Status| resolution[Check Resolution Status]\n            resolution -->|Compile Report| G4\n        end\n    end\n\n    conversion --> getters\n    getters --> log_parsers\n    log_parsers --> metrics\n    metrics --> monitor\n    monitor --> report\n```\nThis mermaid diagram illustrates the interconnected processes and functionalities of the files within the `.docy/docs/json/metrics` folder of the project. It starts with the conversion of log files to ground truth, progresses through getting log file paths, parsing logs based on the testing framework, computing metrics, monitoring validation and log differences, and finally generating reports. Each step is crucial for the workflow, contributing to the project's goal of analyzing and reporting on software evaluation metrics.",
  "questions": "",
  "checksum": "a2c266d204580518ce1485b9ef08b62f"
}