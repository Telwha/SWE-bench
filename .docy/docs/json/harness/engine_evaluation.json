{
  "fileName": "engine_evaluation.py",
  "filePath": "harness/engine_evaluation.py",
  "url": "/blob/master/harness/engine_evaluation.py",
  "summary": "```mermaid\ngraph TD\n    A[Start] --> B{Check Arguments}\n    B --> C[Get Predictions]\n    C --> D{Skip Existing?}\n    D -->|Yes| E[Filter Predictions]\n    D -->|No| F[Split Predictions]\n    E --> F\n    F --> G{Check Num Workers}\n    G -->|One| H[Setup Testbed Single]\n    G -->|Multiple| I[Setup Testbed Parallel]\n    H --> J[End]\n    I --> J\n    \n    subgraph overwrite_ablation\n        OA[Start Overwrite Ablation] --> OB{Check Full Output}\n        OB -->|Missing| OC[Log & Skip]\n        OB -->|Present| OD{Reset Task Env}\n        OD -->|Fail| OE[Log & Skip]\n        OD -->|Success| OF{Run Install Task}\n        OF -->|Fail| OE\n        OF -->|Success| OG{Apply Test Patch}\n        OG -->|Fail| OE\n        OG -->|Success| OH[Overwrite Files]\n        OH --> OI{Run Tests Task}\n        OI -->|Fail| OE\n        OI -->|Success| OJ[End Overwrite Ablation]\n    end\n    \n    subgraph evaluate_predictions\n        EP[Start Evaluate Predictions] --> EQ{Setup Task Env Context}\n        EQ --> ER{Reset Task Env}\n        ER -->|Fail| ES[Continue]\n        ER -->|Success| ET{Apply Prediction Patch}\n        ET -->|Fail| EU[Extract & Apply Minimal Patch]\n        EU -->|Fail| ES\n        EU -->|Success| EV[Revert Patch]\n        ET -->|Success| EV\n        EV --> EW{Run Install & Test}\n        EW -->|Fail| ES\n        EW -->|Success| EX[End Evaluate Predictions]\n    end\n    \n    style overwrite_ablation fill:#f9f,stroke:#333,stroke-width:2px\n    style evaluate_predictions fill:#bbf,stroke:#333,stroke-width:2px\n```\n\nThis mermaid diagram illustrates the flow of the code in the `swe-bench` project, focusing on the low-level operations and how they might be utilized in a larger context. The diagram is divided into main operations, including handling command-line arguments, processing predictions, and the detailed steps within the `overwrite_ablation` and `evaluate_predictions` functions, highlighting their roles in the project's workflow.",
  "questions": "",
  "checksum": "94715ea84776ae30597c0e3759ad8dd1"
}