{
  "folderName": "harness",
  "folderPath": ".docy/docs/json/harness",
  "url": "/tree/master/.docy/docs/json/harness",
  "files": [
    {
      "fileName": "constants.py",
      "filePath": "harness/constants.py",
      "url": "/blob/master/harness/constants.py",
      "summary": "```mermaid\nflowchart TB\n    subgraph swe_bench [\"swe-bench\"]\n    direction TB\n    create_maps[(\"Create Version-to-Install Maps\")]\n    update_maps[(\"Update Version-to-Install Maps with Additional Versions\")]\n    consolidate_maps[(\"Consolidate Maps into MAP_VERSION_TO_INSTALL\")]\n    define_constants[(\"Define Constants for Test Frameworks, Requirement Paths, and Environment Paths\")]\n    define_keys[(\"Define Evaluation Keys\")]\n    define_logging[(\"Define Logging Constants\")]\n    define_misc[(\"Define Miscellaneous Constants\")]\n\n    create_maps --> update_maps\n    update_maps --> consolidate_maps\n    consolidate_maps --> define_constants\n    define_constants --> define_keys\n    define_keys --> define_logging\n    define_logging --> define_misc\n    end\n\n    subgraph larger_project [\"Larger Project\"]\n    direction TB\n    init_env[(\"Initialize Environment Based on MAP_VERSION_TO_INSTALL\")]\n    apply_patches[(\"Apply Patches if Necessary\")]\n    install_dependencies[(\"Install Dependencies\")]\n    run_tests[(\"Run Tests Using Defined Test Frameworks\")]\n    evaluate[(\"Evaluate Test Outcomes\")]\n    log_results[(\"Log Results\")]\n\n    init_env --> apply_patches\n    apply_patches --> install_dependencies\n    install_dependencies --> run_tests\n    run_tests --> evaluate\n    evaluate --> log_results\n    end\n\n    swe_bench -->|Used for| larger_project\n```\nThis flowchart represents the low-level purpose of the code within the `swe-bench` project. It starts with the creation and updating of version-to-install maps for various libraries and frameworks. These maps are then consolidated into a single map (`MAP_VERSION_TO_INSTALL`) which is used throughout the project. Constants for test frameworks, requirement paths, and environment paths are defined, along with evaluation keys and logging constants. Miscellaneous constants are also defined to support the project's operations.\n\nIn the context of the larger project, these maps and constants are utilized to initialize environments specific to different versions of libraries/frameworks, apply necessary patches, install dependencies, run tests using defined frameworks, evaluate test outcomes, and log the results. This process ensures that the `swe-bench` project can dynamically support a wide range of library versions and configurations for testing and evaluation purposes.",
      "questions": "",
      "checksum": "703798e5c4befe7f51f0126a648b55ae"
    },
    {
      "fileName": "context_manager.py",
      "filePath": "harness/context_manager.py",
      "url": "/blob/master/harness/context_manager.py",
      "summary": "```mermaid\nflowchart TB\n    subgraph swe_bench\n    direction TB\n    start([Start]) --> init_logging([Initialize Logging])\n    init_logging --> exec_wrapper([ExecWrapper Class])\n    exec_wrapper --> testbed_context_manager([TestbedContextManager Class])\n    testbed_context_manager --> setup_testbed([Set Up Testbed])\n    setup_testbed --> create_conda_env([Create Conda Environments])\n    create_conda_env --> clone_repo([Clone Repositories])\n    clone_repo --> install_dependencies([Install Dependencies])\n    install_dependencies --> task_env_context_manager([TaskEnvContextManager Class])\n    task_env_context_manager --> enter_task_env([Enter Task Environment])\n    enter_task_env --> reset_task_env([Reset Task Environment])\n    reset_task_env --> run_install_task([Run Installation Task])\n    run_install_task --> apply_patch([Apply Patch])\n    apply_patch --> run_tests_task([Run Tests Task])\n    run_tests_task --> exit_task_env([Exit Task Environment])\n    exit_task_env --> end([End])\n    end\n    end\n```\nThis flowchart represents the sequence of operations performed by the code in the `swe-bench` project. It starts with initializing logging, followed by the execution of the `ExecWrapper` class for running subprocess commands with specific arguments. The `TestbedContextManager` class sets up the testbed, including creating Conda environments, cloning repositories, and installing dependencies. The `TaskEnvContextManager` class manages the execution context for individual task instances, including entering and exiting the task environment, resetting the environment, running installation tasks, applying patches, and running test tasks.",
      "questions": "",
      "checksum": "21febd3e23710a9e08e8aa60e4027a87"
    },
    {
      "fileName": "engine_evaluation.py",
      "filePath": "harness/engine_evaluation.py",
      "url": "/blob/master/harness/engine_evaluation.py",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B{Check Arguments}\n    B --> C[Get Predictions]\n    C --> D{Skip Existing?}\n    D -->|Yes| E[Filter Predictions]\n    D -->|No| F[Split Predictions]\n    E --> F\n    F --> G{Check Num Workers}\n    G -->|One| H[Setup Testbed Single]\n    G -->|Multiple| I[Setup Testbed Parallel]\n    H --> J[End]\n    I --> J\n    \n    subgraph overwrite_ablation\n        OA[Start Overwrite Ablation] --> OB{Check Full Output}\n        OB -->|Missing| OC[Log & Skip]\n        OB -->|Present| OD{Reset Task Env}\n        OD -->|Fail| OE[Log & Skip]\n        OD -->|Success| OF{Run Install Task}\n        OF -->|Fail| OE\n        OF -->|Success| OG{Apply Test Patch}\n        OG -->|Fail| OE\n        OG -->|Success| OH[Overwrite Files]\n        OH --> OI{Run Tests Task}\n        OI -->|Fail| OE\n        OI -->|Success| OJ[End Overwrite Ablation]\n    end\n    \n    subgraph evaluate_predictions\n        EP[Start Evaluate Predictions] --> EQ{Setup Task Env Context}\n        EQ --> ER{Reset Task Env}\n        ER -->|Fail| ES[Continue]\n        ER -->|Success| ET{Apply Prediction Patch}\n        ET -->|Fail| EU[Extract & Apply Minimal Patch]\n        EU -->|Fail| ES\n        EU -->|Success| EV[Revert Patch]\n        ET -->|Success| EV\n        EV --> EW{Run Install & Test}\n        EW -->|Fail| ES\n        EW -->|Success| EX[End Evaluate Predictions]\n    end\n    \n    style overwrite_ablation fill:#f9f,stroke:#333,stroke-width:2px\n    style evaluate_predictions fill:#bbf,stroke:#333,stroke-width:2px\n```\n\nThis mermaid diagram illustrates the flow of the code in the `swe-bench` project, focusing on the low-level operations and how they might be utilized in a larger context. The diagram is divided into main operations, including handling command-line arguments, processing predictions, and the detailed steps within the `overwrite_ablation` and `evaluate_predictions` functions, highlighting their roles in the project's workflow.",
      "questions": "",
      "checksum": "94715ea84776ae30597c0e3759ad8dd1"
    },
    {
      "fileName": "engine_validation.py",
      "filePath": "harness/engine_validation.py",
      "url": "/blob/master/harness/engine_validation.py",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B{Validate Command Line Arguments}\n    B --> C[Load Task Instances]\n    C --> D[Split Task Instances]\n    D --> E{Check Number of Workers}\n    E -->|One Worker| F[Setup Testbed for Single Worker]\n    E -->|Multiple Workers| G[Setup Testbed for Multiple Workers]\n    F --> H[Verify Task Instances Sequentially]\n    G --> I[Verify Task Instances in Parallel]\n    H --> J[End]\n    I --> J\n    F --> K[Create Testbed Context]\n    G --> L[Create Multiple Testbed Contexts]\n    K --> M[Run Tasks Sequentially in Context]\n    L --> N[Run Tasks in Parallel across Contexts]\n    M --> O[Task Environment Setup]\n    N --> O\n    O --> P{Check if Task Instance Should be Skipped}\n    P -->|Yes| Q[Skip Task Instance]\n    P -->|No| R[Reset Task Environment]\n    R --> S[Run Install Task]\n    S --> T[Apply Test Patch]\n    T --> U[Run Tests Task (Test Patch)]\n    U --> V[Apply Gold Patch]\n    V --> W[Run Tests Task (Gold Patch)]\n    Q --> X[Continue to Next Task Instance]\n    W --> X\n    X -->|All Task Instances Processed| J\n```\nThis mermaid diagram illustrates the flow of operations within the provided code, focusing on the validation of command line arguments, the handling of task instances (including their distribution among workers), and the detailed steps taken to verify each task instance within its environment.",
      "questions": "",
      "checksum": "551278ec463748561e0eb8a2fc9a6c8b"
    },
    {
      "fileName": "run_evaluation.py",
      "filePath": "harness/run_evaluation.py",
      "url": "/blob/master/harness/run_evaluation.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Validate Arguments}\n    B -->|Invalid| C[Raise ValueError]\n    B -->|Valid| D[Load Tasks]\n    D --> E{Tasks Source}\n    E -->|Local Path| F[Load from Path]\n    E -->|dev/test| G[Load from HuggingFace Datasets]\n    F --> H[Verify Tasks Format]\n    G --> H\n    H --> I[Validate Predictions]\n    I --> J[Group Predictions by Model]\n    J --> K[For Each Model]\n    K --> L[Group Predictions by Repo and Version]\n    L --> M[For Each Repo/Version]\n    M --> N[Create Testbed Folder]\n    N --> O[Save Predictions to File]\n    O --> P{Skip Existing Checks}\n    P -->|Yes| Q[Filter Already Evaluated Predictions]\n    P -->|No| R[Proceed Without Filtering]\n    Q --> S[Log Info & Skip if All Exist]\n    R --> S\n    S --> T{Any Predictions to Evaluate?}\n    T -->|No| U[Log Info & Exit]\n    T -->|Yes| V[Prepare Evaluation Arguments]\n    V --> W{Run Evaluation}\n    W -->|Single Process| X[Sequential Evaluation]\n    W -->|Multiple Processes| Y[Parallel Evaluation]\n    X --> Z[Clean Up]\n    Y --> Z\n    Z --> AA[End]\n```",
      "questions": "",
      "checksum": "7d826297051deff10518210835f311aa"
    },
    {
      "fileName": "run_validation.sh",
      "filePath": "harness/run_validation.sh",
      "url": "/blob/master/harness/run_validation.sh",
      "summary": "```mermaid\ngraph TD\n    A[Start engine_validation.py] --> B{Check if instances_path, log_dir, and temp_dir are provided}\n    B -->|Yes| C[Initialize with provided paths]\n    B -->|No| Z[Exit with error: Paths not provided]\n    C --> D{Check if num_workers is provided}\n    D -->|Yes| E[Set number of workers]\n    D -->|No| F[Use default number of workers]\n    E --> G[Start validation process]\n    F --> G\n    G --> H{Is verbose mode on?}\n    H -->|Yes| I[Run in verbose mode: Show detailed logs]\n    H -->|No| J[Run in silent mode: Show minimal logs]\n    I --> K[Perform validation on task instances]\n    J --> K\n    K --> L{Validation Complete}\n    L --> M[Generate and save logs to log_dir]\n    L --> N[Temporary files saved to temp_dir]\n    M --> O[End of engine_validation.py]\n    N --> O\n```\nThis flowchart represents the steps executed by the `engine_validation.py` script within the context of its operational environment, focusing on the initialization, configuration, and execution phases, including error handling and output management.",
      "questions": "",
      "checksum": "b5853d9316c54cfed4b78577dfc59a53"
    },
    {
      "fileName": "utils.py",
      "filePath": "harness/utils.py",
      "url": "/blob/master/harness/utils.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Load Environment Variables}\n    B --> C[Get Task Instances]\n    C --> D{For Each Instance}\n    D --> E[Get Conda Environment Names]\n    E --> F[Get Environment YML]\n    F --> G[Get Requirements.txt]\n    G --> H[Get Test Directives]\n    H --> I[Clone Repo]\n    I --> J[Split Instances]\n    J --> K[Find Python Version by Date]\n    K --> L[Extract Minimal Patch]\n    L --> M[Check for Attribute or Import Error]\n    M --> N[End]\n\n    click E \"Get list of conda environment names for given conda path\"\n    click F \"Get environment.yml for given task instance\"\n    click G \"Get requirements.txt for given task instance\"\n    click H \"Get test directives from the test_patch of a task instance\"\n    click I \"Wrapper for cloning repo from swe-bench organization\"\n    click J \"Split a list into n approximately equal length sublists\"\n    click K \"Find python version closest to given date\"\n    click L \"Wrapper function that takes hunk and recalculates hunk start/end position and diff delta\"\n    click M \"Check to see if Attribute/Import-prefix is in log text\"\n```\nThis flowchart represents the sequence of operations performed by the code in the `swe-bench` project file. Each node represents a function or a step in the process, illustrating how the code manages environment variables, interacts with task instances, and processes information related to Python environments, requirements, and repositories.",
      "questions": "",
      "checksum": "c05e5c3591deff056f0125955626075f"
    }
  ],
  "folders": [],
  "summary": "```mermaid\nflowchart TB\n    subgraph constants_section[\"constants.py\"]\n    create_maps((\"Create Version-to-Install Maps\")) --> update_maps((\"Update Version-to-Install Maps\"))\n    update_maps --> consolidate_maps((\"Consolidate Maps\"))\n    consolidate_maps --> define_constants((\"Define Constants\"))\n    define_constants --> define_keys((\"Define Evaluation Keys\"))\n    define_keys --> define_logging((\"Define Logging Constants\"))\n    define_logging --> define_misc((\"Define Miscellaneous Constants\"))\n    end\n\n    subgraph context_manager_section[\"context_manager.py\"]\n    start((\"Start\")) --> init_logging((\"Initialize Logging\"))\n    init_logging --> exec_wrapper((\"ExecWrapper Class\"))\n    exec_wrapper --> testbed_context_manager((\"TestbedContextManager Class\"))\n    testbed_context_manager --> setup_testbed((\"Set Up Testbed\"))\n    setup_testbed --> create_conda_env((\"Create Conda Environments\"))\n    create_conda_env --> clone_repo((\"Clone Repositories\"))\n    clone_repo --> install_dependencies_cm((\"Install Dependencies\"))\n    install_dependencies_cm --> task_env_context_manager((\"TaskEnvContextManager Class\"))\n    task_env_context_manager --> enter_task_env((\"Enter Task Environment\"))\n    enter_task_env --> reset_task_env((\"Reset Task Environment\"))\n    reset_task_env --> run_install_task_cm((\"Run Installation Task\"))\n    run_install_task_cm --> apply_patch_cm((\"Apply Patch\"))\n    apply_patch_cm --> run_tests_task_cm((\"Run Tests Task\"))\n    run_tests_task_cm --> exit_task_env((\"Exit Task Environment\"))\n    exit_task_env --> end_cm((\"End\"))\n    end\n\n    subgraph engine_evaluation_section[\"engine_evaluation.py\"]\n    check_arguments((\"Check Arguments\")) --> get_predictions((\"Get Predictions\"))\n    get_predictions --> skip_existing((\"Skip Existing?\"))\n    skip_existing -->|Yes| filter_predictions((\"Filter Predictions\"))\n    skip_existing -->|No| split_predictions((\"Split Predictions\"))\n    filter_predictions --> split_predictions\n    split_predictions --> check_num_workers((\"Check Num Workers\"))\n    check_num_workers -->|One| setup_testbed_single((\"Setup Testbed Single\"))\n    check_num_workers -->|Multiple| setup_testbed_parallel((\"Setup Testbed Parallel\"))\n    setup_testbed_single --> overwrite_ablation((\"Overwrite Ablation\"))\n    setup_testbed_parallel --> evaluate_predictions((\"Evaluate Predictions\"))\n    end\n\n    subgraph engine_validation_section[\"engine_validation.py\"]\n    validate_cli_args((\"Validate CLI Arguments\")) --> load_task_instances((\"Load Task Instances\"))\n    load_task_instances --> split_task_instances((\"Split Task Instances\"))\n    split_task_instances --> check_num_workers_ev((\"Check Number of Workers\"))\n    check_num_workers_ev -->|One Worker| setup_testbed_single_ev((\"Setup Testbed for Single Worker\"))\n    check_num_workers_ev -->|Multiple Workers| setup_testbed_parallel_ev((\"Setup Testbed for Multiple Workers\"))\n    setup_testbed_single_ev --> verify_task_instances_seq((\"Verify Task Instances Sequentially\"))\n    setup_testbed_parallel_ev --> verify_task_instances_par((\"Verify Task Instances in Parallel\"))\n    verify_task_instances_seq --> create_testbed_context((\"Create Testbed Context\"))\n    verify_task_instances_par --> create_multiple_testbed_contexts((\"Create Multiple Testbed Contexts\"))\n    create_testbed_context --> run_tasks_seq_in_context((\"Run Tasks Sequentially in Context\"))\n    create_multiple_testbed_contexts --> run_tasks_par_across_contexts((\"Run Tasks in Parallel across Contexts\"))\n    run_tasks_seq_in_context --> task_env_setup((\"Task Environment Setup\"))\n    run_tasks_par_across_contexts --> task_env_setup\n    task_env_setup --> check_skip_task_instance((\"Check if Task Instance Should be Skipped\"))\n    check_skip_task_instance -->|Yes| skip_task_instance((\"Skip Task Instance\"))\n    check_skip_task_instance -->|No| reset_task_env_ev((\"Reset Task Environment\"))\n    reset_task_env_ev --> run_install_task_ev((\"Run Install Task\"))\n    run_install_task_ev --> apply_test_patch((\"Apply Test Patch\"))\n    apply_test_patch --> run_tests_task_test_patch((\"Run Tests Task (Test Patch)\"))\n    run_tests_task_test_patch --> apply_gold_patch((\"Apply Gold Patch\"))\n    apply_gold_patch --> run_tests_task_gold_patch((\"Run Tests Task (Gold Patch)\"))\n    skip_task_instance --> continue_next_task_instance((\"Continue to Next Task Instance\"))\n    run_tests_task_gold_patch --> continue_next_task_instance\n    continue_next_task_instance --> end_ev((\"End\"))\n    end\n\n    subgraph run_evaluation_section[\"run_evaluation.py\"]\n    validate_arguments_re((\"Validate Arguments\")) --> load_tasks((\"Load Tasks\"))\n    load_tasks --> tasks_source((\"Tasks Source\"))\n    tasks_source -->|Local Path| load_from_path((\"Load from Path\"))\n    tasks_source -->|dev/test| load_from_hf_datasets((\"Load from HuggingFace Datasets\"))\n    load_from_path --> verify_tasks_format((\"Verify Tasks Format\"))\n    load_from_hf_datasets --> verify_tasks_format\n    verify_tasks_format --> validate_predictions_re((\"Validate Predictions\"))\n    validate_predictions_re --> group_predictions_by_model((\"Group Predictions by Model\"))\n    group_predictions_by_model --> for_each_model((\"For Each Model\"))\n    for_each_model --> group_predictions_by_repo_and_version((\"Group Predictions by Repo and Version\"))\n    group_predictions_by_repo_and_version --> for_each_repo_version((\"For Each Repo/Version\"))\n    for_each_repo_version --> create_testbed_folder((\"Create Testbed Folder\"))\n    create_testbed_folder --> save_predictions_to_file((\"Save Predictions to File\"))\n    save_predictions_to_file --> skip_existing_checks_re((\"Skip Existing Checks\"))\n    skip_existing_checks_re -->|Yes| filter_already_evaluated_predictions((\"Filter Already Evaluated Predictions\"))\n    skip_existing_checks_re -->|No| proceed_without_filtering((\"Proceed Without Filtering\"))\n    filter_already_evaluated_predictions --> log_info_skip_if_all_exist((\"Log Info & Skip if All Exist\"))\n    proceed_without_filtering --> log_info_skip_if_all_exist\n    log_info_skip_if_all_exist --> any_predictions_to_evaluate((\"Any Predictions to Evaluate?\"))\n    any_predictions_to_evaluate -->|No| log_info_exit((\"Log Info & Exit\"))\n    any_predictions_to_evaluate -->|Yes| prepare_evaluation_arguments((\"Prepare Evaluation Arguments\"))\n    prepare_evaluation_arguments --> run_evaluation_re((\"Run Evaluation\"))\n    run_evaluation_re -->|Single Process| sequential_evaluation((\"Sequential Evaluation\"))\n    run_evaluation_re -->|Multiple Processes| parallel_evaluation((\"Parallel Evaluation\"))\n    sequential_evaluation --> clean_up_re((\"Clean Up\"))\n    parallel_evaluation --> clean_up_re\n    clean_up_re --> end_re((\"End\"))\n    end\n\n    subgraph run_validation_section[\"run_validation.sh\"]\n    start_ev_script((\"Start engine_validation.py\")) --> check_paths_provided((\"Check if paths are provided\"))\n    check_paths_provided -->|Yes| initialize_with_provided_paths((\"Initialize with provided paths\"))\n    check_paths_provided -->|No| exit_with_error((\"Exit with error: Paths not provided\"))\n    initialize_with_provided_paths --> check_num_workers_rv((\"Check if num_workers is provided\"))\n    check_num_workers_rv -->|Yes| set_number_of_workers((\"Set number of workers\"))\n    check_num_workers_rv -->|No| use_default_number_of_workers((\"Use default number of workers\"))\n    set_number_of_workers --> start_validation_process((\"Start validation process\"))\n    use_default_number_of_workers --> start_validation_process\n    start_validation_process --> is_verbose_mode_on((\"Is verbose mode on?\"))\n    is_verbose_mode_on -->|Yes| run_in_verbose_mode((\"Run in verbose mode\"))\n    is_verbose_mode_on -->|No| run_in_silent_mode((\"Run in silent mode\"))\n    run_in_verbose_mode --> perform_validation_on_task_instances((\"Perform validation on task instances\"))\n    run_in_silent_mode --> perform_validation_on_task_instances\n    perform_validation_on_task_instances --> validation_complete((\"Validation Complete\"))\n    validation_complete --> generate_and_save_logs_to_log_dir((\"Generate and save logs\"))\n    validation_complete --> temporary_files_saved_to_temp_dir((\"Temporary files saved\"))\n    generate_and_save_logs_to_log_dir --> end_of_ev_script((\"End of engine_validation.py\"))\n    temporary_files_saved_to_temp_dir --> end_of_ev_script\n    end\n\n    subgraph utils_section[\"utils.py\"]\n    load_env_vars((\"Load Environment Variables\")) --> get_task_instances_u((\"Get Task Instances\"))\n    get_task_instances_u --> for_each_instance((\"For Each Instance\"))\n    for_each_instance --> get_conda_env_names((\"Get Conda Environment Names\"))\n    get_conda_env_names --> get_environment_yml((\"Get Environment YML\"))\n    get_environment_yml --> get_requirements_txt((\"Get Requirements.txt\"))\n    get_requirements_txt --> get_test_directives((\"Get Test Directives\"))\n    get_test_directives --> clone_repo_u((\"Clone Repo\"))\n    clone_repo_u --> split_instances((\"Split Instances\"))\n    split_instances --> find_python_version_by_date((\"Find Python Version by Date\"))\n    find_python_version_by_date --> extract_minimal_patch((\"Extract Minimal Patch\"))\n    extract_minimal_patch --> check_for_attribute_or_import_error((\"Check for Attribute or Import Error\"))\n    check_for_attribute_or_import_error --> end_u((\"End\"))\n    end\n\n    constants_section --> context_manager_section\n    context_manager_section --> engine_evaluation_section\n    engine_evaluation_section --> engine_validation_section\n    engine_validation_section --> run_evaluation_section\n    run_evaluation_section --> run_validation_section\n    run_validation_section --> utils_section\n```",
  "questions": "",
  "checksum": "084ec71b8d69b7a0452e09755b2aeebf"
}