{
  "folderName": "make_datasets",
  "folderPath": ".docy/docs/json/inference/make_datasets",
  "url": "/tree/master/.docy/docs/json/inference/make_datasets",
  "files": [
    {
      "fileName": "__init__.py",
      "filePath": "inference/make_datasets/__init__.py",
      "url": "/blob/master/inference/make_datasets/__init__.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Benchmark Environment]\n    B --> C{Check if Benchmark Data Exists}\n    C -->|Yes| D[Load Existing Benchmark Data]\n    C -->|No| E[Generate New Benchmark Data]\n    D --> F[Prepare Benchmark Tests]\n    E --> F\n    F --> G{Is Benchmark Configuration Valid?}\n    G -->|Yes| H[Run Benchmark Tests]\n    G -->|No| I[Log Configuration Error]\n    H --> J[Collect Benchmark Results]\n    J --> K{Are Results Within Expected Range?}\n    K -->|Yes| L[Store Benchmark Results]\n    K -->|No| M[Log Results Out of Range Error]\n    L --> N[Generate Benchmark Report]\n    M --> N\n    N --> O[End]\n```\nThis flowchart represents the steps executed by the code in the `swe-bench` project. It starts with initializing the benchmark environment, followed by checking for existing benchmark data. Depending on whether the data exists, it either loads the existing data or generates new data. After preparing the benchmark tests, it checks if the benchmark configuration is valid. If valid, it runs the benchmark tests; otherwise, it logs a configuration error. Post running the tests, it collects the results and checks if they are within the expected range. If the results are as expected, it stores them and generates a report. If not, it logs an error indicating the results are out of the expected range before generating the report. The process ends after the report generation.",
      "questions": "",
      "checksum": "74be16979710d4c4e7c6647856088456"
    },
    {
      "fileName": "bm25_retrieval.py",
      "filePath": "inference/make_datasets/bm25_retrieval.py",
      "url": "/blob/master/inference/make_datasets/bm25_retrieval.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Check if dataset path exists}\n    B -- Yes --> C[Load dataset from disk]\n    B -- No --> D[Load dataset from HuggingFace Datasets]\n    C --> E[Prepare dataset splits]\n    D --> E\n    E --> F{Check for unknown splits}\n    F -- Yes --> G[Error: Unknown splits]\n    F -- No --> H[Get remaining instances]\n    H --> I[Get root directory and name]\n    I --> J[Get index paths for instances]\n    J --> K[Search indexes for instances]\n    K --> L[Get missing IDs]\n    L --> M[Cleanup and finish]\n    M --> N[End]\n```\n\nThis flowchart represents the process outlined in the provided code, focusing on the main steps involved in preparing, processing, and cleaning up after searching indexes for instances in a dataset. The process starts with checking if the dataset path exists, leading to either loading the dataset from disk or from HuggingFace Datasets. After preparing the dataset splits, it checks for unknown splits, which could lead to an error. Then, it proceeds to get the remaining instances that need processing, prepares the root directory, retrieves index paths for instances, and performs searches on these indexes. Finally, it identifies any missing IDs, cleans up resources, and finishes the process.",
      "questions": "",
      "checksum": "f2d5e1f43580981a726649c04dc2a35b"
    },
    {
      "fileName": "create_instance.py",
      "filePath": "inference/make_datasets/create_instance.py",
      "url": "/blob/master/inference/make_datasets/create_instance.py",
      "summary": "```mermaid\ngraph TD\n    A(Start) --> B[Import Libraries and Modules]\n    B --> C[Define Constants and Load External Resources]\n    C --> D[Define Utility Functions]\n    D --> E{Check for File Source}\n    E -->|oracle| F[Ingest Files from Oracle Filenames]\n    E -->|bm25| G[Add Retrieval Results]\n    E -->|all| H[Ingest All Directory Contents]\n    E -->|none| I[Set File Contents to Empty]\n    G --> J[Ingest Files from Retrieval Hits]\n    F --> K[Generate Prompt Based on Style]\n    J --> K\n    H --> K\n    I --> K\n    K --> L[Add Text Inputs to Instances]\n    L --> M[Handle Context Length Limitation]\n    M --> N[Finalize Text Inputs]\n    N --> O[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style O fill:#f9f,stroke:#333,stroke-width:4px\n```",
      "questions": "",
      "checksum": "6fad6e6e19ea4bb02d7f8ed4211f6d0d"
    },
    {
      "fileName": "create_text_dataset.py",
      "filePath": "inference/make_datasets/create_text_dataset.py",
      "url": "/blob/master/inference/make_datasets/create_text_dataset.py",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Parse Arguments]\n    B --> C{Check push_to_hub_user}\n    C -->|None| D[Create output directory if not exists]\n    C -->|Not None| E[Check HUGGING_FACE_HUB_TOKEN and output_dir]\n    B --> F{Check max_context_len}\n    F -->|Not None| G[Assert tokenizer_name is not None]\n    B --> H[Generate output file name based on arguments]\n    H --> I{Check if dataset exists on disk}\n    I -->|Yes| J[Load dataset from disk]\n    I -->|No| K[Load dataset from HuggingFace Datasets]\n    K --> L[Process each split in dataset]\n    L --> M[Add text inputs to instances]\n    M --> N[Extract fields from instances]\n    N --> O[Create Dataset from processed data]\n    O --> P{Check validation_ratio}\n    P -->|>0| Q[Split train dataset into train and validation]\n    P -->|<=0| R[Proceed without splitting]\n    Q --> S[Log number of instances per split]\n    R --> S\n    S --> T{Check push_to_hub_user}\n    T -->|Not None| U[Push dataset to HuggingFace Hub]\n    T -->|None| V[Save dataset to disk]\n    U --> W[End]\n    V --> W\n```\nThis mermaid diagram illustrates the flow of operations in the provided code snippet. It starts with parsing command-line arguments, checks conditions related to `push_to_hub_user` and `max_context_len`, and decides on the dataset loading strategy. It then processes each dataset split by adding text inputs, extracting necessary fields, and creating a `Dataset` object. Depending on the `validation_ratio`, it may split the training dataset. Finally, it either pushes the dataset to the HuggingFace Hub or saves it to disk, based on the `push_to_hub_user` argument.",
      "questions": "",
      "checksum": "1a24470e01024da31b02a08e1b4b66b3"
    },
    {
      "fileName": "tokenize_dataset.py",
      "filePath": "inference/make_datasets/tokenize_dataset.py",
      "url": "/blob/master/inference/make_datasets/tokenize_dataset.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Check if tokenizer name provided}\n    B -->|Yes| C[Initialize tokenizer and function]\n    B -->|No| Z[End without tokenizer initialization]\n    C --> D{Check if dataset path exists}\n    D -->|Yes| E[Load dataset from disk]\n    D -->|No| F[Load dataset using name or path]\n    E & F --> G[Filter out superlong instances]\n    G --> H{Is split test?}\n    H -->|No| I[Tokenize non-test split]\n    H -->|Yes| J[Skip tokenizing test split]\n    I --> K{Is num_proc > 0?}\n    K -->|Yes| L[Tokenize with multiprocessing]\n    K -->|No| M[Tokenize without multiprocessing]\n    L & M --> N[Add tokenized columns to dataset]\n    J --> O{Is num_proc > 0?}\n    O -->|Yes| P[Tokenize test split with multiprocessing]\n    O -->|No| Q[Tokenize test split without multiprocessing]\n    P & Q --> R[Add tokenized columns to test dataset]\n    N & R --> S{Is push to hub user specified?}\n    S -->|Yes| T[Push dataset to Hugging Face Hub]\n    S -->|No| U[Save dataset to disk]\n    T & U --> Z[End]\n```",
      "questions": "",
      "checksum": "fed2d027705839539fd984bd10274e6d"
    },
    {
      "fileName": "utils.py",
      "filePath": "inference/make_datasets/utils.py",
      "url": "/blob/master/inference/make_datasets/utils.py",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Extract Diff]\n    B --> C{Is Test?}\n    C -->|Yes| D[Skip File]\n    C -->|No| E[Detect Encoding]\n    E --> F[Read File Content]\n    F --> G[Ingest Directory Contents]\n    G --> H[Ingest File Directory Contents]\n    H --> I[Repair Patch]\n    I --> J[Extract Minimal Patch]\n    J --> K[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style K fill:#f9f,stroke:#333,stroke-width:4px\n```\nThis flowchart represents the sequence of operations performed by the code in the provided context. It starts with extracting diffs from a given input, checks if the file is a test file, detects the file encoding, reads the file content, ingests directory contents, ingests file directory contents, repairs patches, and finally extracts a minimal patch from the given input. The process begins at \"Start\" and ends at \"End\", with decision points and operations represented as nodes in the flowchart.",
      "questions": "",
      "checksum": "4522388584c3bb89a5872a909821cea4"
    }
  ],
  "folders": [],
  "summary": "```mermaid\nflowchart TB\n    subgraph swe_bench [\" \"]\n    start((Start))\n    init[(\"/__init__.py\\nInitialize Benchmark Environment\")]\n    bm25[(\"/bm25_retrieval.py\\nBM25 Dataset Retrieval\")]\n    create_inst[(\"/create_instance.py\\nCreate Instance\")]\n    create_text_ds[(\"/create_text_dataset.py\\nCreate Text Dataset\")]\n    tokenize_ds[(\"/tokenize_dataset.py\\nTokenize Dataset\")]\n    utils[(\"/utils.py\\nUtility Functions\")]\n    end((End))\n\n    start --> init\n    init --> bm25\n    init --> create_inst\n    init --> create_text_ds\n    init --> tokenize_ds\n    init --> utils\n    bm25 --> create_text_ds\n    create_inst --> create_text_ds\n    tokenize_ds --> create_text_ds\n    utils --> create_inst\n    utils --> bm25\n    create_text_ds --> end\n    end\n    end\n```\n\nThis diagram illustrates the interconnected steps and processes within the `make_datasets` folder of the project. It starts with initializing the benchmark environment, followed by various processes such as BM25 dataset retrieval, instance creation, text dataset creation, dataset tokenization, and utility functions. Each script contributes to the preparation and processing of datasets, ultimately leading to the creation of a text dataset that is ready for further actions, such as benchmarking or analysis. Utility functions support various steps in the process, indicating a cohesive workflow aimed at dataset preparation and processing within the project.",
  "questions": "",
  "checksum": "0f34dfbe0d073e32b17eef7664627f7d"
}