{
  "fileName": "create_text_dataset.py",
  "filePath": "inference/make_datasets/create_text_dataset.py",
  "url": "/blob/master/inference/make_datasets/create_text_dataset.py",
  "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Parse Arguments]\n    B --> C{Check push_to_hub_user}\n    C -->|None| D[Create output directory if not exists]\n    C -->|Not None| E[Check HUGGING_FACE_HUB_TOKEN and output_dir]\n    B --> F{Check max_context_len}\n    F -->|Not None| G[Assert tokenizer_name is not None]\n    B --> H[Generate output file name based on arguments]\n    H --> I{Check if dataset exists on disk}\n    I -->|Yes| J[Load dataset from disk]\n    I -->|No| K[Load dataset from HuggingFace Datasets]\n    K --> L[Process each split in dataset]\n    L --> M[Add text inputs to instances]\n    M --> N[Extract fields from instances]\n    N --> O[Create Dataset from processed data]\n    O --> P{Check validation_ratio}\n    P -->|>0| Q[Split train dataset into train and validation]\n    P -->|<=0| R[Proceed without splitting]\n    Q --> S[Log number of instances per split]\n    R --> S\n    S --> T{Check push_to_hub_user}\n    T -->|Not None| U[Push dataset to HuggingFace Hub]\n    T -->|None| V[Save dataset to disk]\n    U --> W[End]\n    V --> W\n```\nThis mermaid diagram illustrates the flow of operations in the provided code snippet. It starts with parsing command-line arguments, checks conditions related to `push_to_hub_user` and `max_context_len`, and decides on the dataset loading strategy. It then processes each dataset split by adding text inputs, extracting necessary fields, and creating a `Dataset` object. Depending on the `validation_ratio`, it may split the training dataset. Finally, it either pushes the dataset to the HuggingFace Hub or saves it to disk, based on the `push_to_hub_user` argument.",
  "questions": "",
  "checksum": "1a24470e01024da31b02a08e1b4b66b3"
}