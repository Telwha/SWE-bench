{
  "folderName": "inference",
  "folderPath": ".docy/docs/json/inference",
  "url": "/tree/master/.docy/docs/json/inference",
  "files": [
    {
      "fileName": "__init__.py",
      "filePath": "inference/__init__.py",
      "url": "/blob/master/inference/__init__.py",
      "summary": "```markdown\n```mermaid\ngraph TD\n    A[Start] --> B[Initialize swe-bench Environment]\n    B --> C{Check if Data Exists}\n    C -->|Yes| D[Load Existing Data]\n    C -->|No| E[Create New Data]\n    D --> F[Process Data]\n    E --> F\n    F --> G{Is Processing Complete?}\n    G -->|Yes| H[Save Processed Data]\n    G -->|No| F\n    H --> I[Generate Reports]\n    I --> J[End]\n```\n```",
      "questions": "",
      "checksum": "74be16979710d4c4e7c6647856088456"
    },
    {
      "fileName": "codellama_device_maps.json",
      "filePath": "inference/codellama_device_maps.json",
      "url": "/blob/master/inference/codellama_device_maps.json",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Initialize Model Components]\n    B --> C{Component Type}\n    C -->|Embed Tokens| D[Embedding Layer]\n    C -->|Model Layers| E[Processing Layers]\n    C -->|Norm| F[Normalization Layer]\n    C -->|LM Head| G[Language Model Head]\n    D --> H[Token Embedding Processing]\n    E --> I[Layer-wise Processing]\n    F --> J[Normalization Processing]\n    G --> K[Language Model Output]\n    I --> L{Layer Activation}\n    L -->|0| M[No Activation]\n    L -->|1| N[Activation Level 1]\n    L -->|2| O[Activation Level 2]\n    L -->|3| P[Activation Level 3]\n    M --> Q[Output to Next Layer/Process]\n    N --> Q\n    O --> Q\n    P --> Q\n    Q --> R{Next Component}\n    R -->|Model Layers| E\n    R -->|Norm| F\n    R -->|LM Head| G\n    R -->|End| S[End of Model Processing]\n```\nThis mermaid diagram represents the flow of processing within a model, starting from initialization, through different types of components (embedding, layers, normalization, and language model head), and detailing how each layer's activation level affects the processing flow.",
      "questions": "",
      "checksum": "a2eb26fa0fc94ed2cdf0a8defa0756e1"
    },
    {
      "fileName": "environment.yml",
      "filePath": "inference/environment.yml",
      "url": "/blob/master/inference/environment.yml",
      "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Define Channels]\n    B --> C[Define Dependencies]\n    C --> D[Base Environment Dependencies]\n    D --> E[Python and Core Libraries]\n    E --> F[Development Tools]\n    F --> G[Define pip Dependencies]\n    G --> H[Core Python Libraries]\n    H --> I[Data Processing and AI Libraries]\n    I --> J[Web and Networking Libraries]\n    J --> K[Development and Debugging Tools]\n    K --> L[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style L fill:#f9f,stroke:#333,stroke-width:4px\n```",
      "questions": "",
      "checksum": "07fb322484f3d9c3786e8a399da0569c"
    },
    {
      "fileName": "run_api.py",
      "filePath": "inference/run_api.py",
      "url": "/blob/master/inference/run_api.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Check Model Prefix}\n    B -->|claude| C[Anthropic Inference]\n    B -->|gpt| D[OpenAI Inference]\n    C --> E[Load Dataset]\n    D --> E\n    E --> F{Filter Dataset}\n    F -->|Existing IDs| G[Filter out existing IDs]\n    F -->|Shard Info| H[Select Shard]\n    G --> H\n    H --> I[Inference Loop]\n    I --> J{Check Max Cost}\n    J -->|Reached| K[Exit]\n    J -->|Not Reached| I\n    K --> L[End]\n```\n\nThis flowchart represents the high-level logic of the provided code. The process starts with determining the model prefix to decide whether to use the Anthropic or OpenAI inference path. After loading and optionally filtering the dataset based on existing IDs and shard information, it enters an inference loop. Within this loop, it checks if the maximum cost has been reached after each inference call. If the max cost is reached, the process exits; otherwise, it continues with the next inference call until completion.",
      "questions": "",
      "checksum": "ca18f1d1f48bde28825453b787153850"
    },
    {
      "fileName": "run_live.py",
      "filePath": "inference/run_live.py",
      "url": "/blob/master/inference/run_live.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Parse Issue URL}\n    B -->|Valid| C[Get Problem Statement]\n    B -->|Invalid| Z[Error: Invalid URL]\n    C --> D[Clone Repository]\n    D --> E[Build BM25 Retrieval Index]\n    E --> F[Search in Index]\n    F --> G{Include READMEs?}\n    G -->|Yes| H[Get README Files]\n    G -->|No| I[Skip READMEs]\n    H --> J[Ingest Files]\n    I --> J\n    J --> K[Generate Prompt]\n    K --> L[Call Model]\n    L --> M[Extract Diff from Response]\n    M --> N[Extract Minimal Patch]\n    N --> O[Save Output]\n    O --> P[End]\n```\nThis flowchart represents the steps taken by the code to run a live inference session on a GitHub issue. It starts with parsing the issue URL, retrieving the problem statement, cloning the repository, and building a BM25 retrieval index. It then searches the index, decides whether to include README files, ingests files if necessary, and generates a prompt. The model is called with this prompt, and the response is processed to extract a diff and a minimal patch. Finally, the output is saved, and the process ends.",
      "questions": "",
      "checksum": "cb87d8322f8e6d2f5b897b798132bc3f"
    },
    {
      "fileName": "run_llama.py",
      "filePath": "inference/run_llama.py",
      "url": "/blob/master/inference/run_llama.py",
      "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Check Shard Info}\n    B -->|Valid| C[Load PEFT Config]\n    B -->|Invalid| D[Error: Shard Info Mismatch]\n    C --> E[Construct Output File Path]\n    E --> F[Load Model]\n    F --> G[Load Tokenizer]\n    G --> H[Get All Existing IDs]\n    H --> I[Load and Preprocess Data]\n    I --> J[Generate Patches]\n    J --> K[End]\n\n    click B \"Check if shard_id and num_shards are specified correctly.\"\n    click C \"Load PEFT configuration if peft_path is provided.\"\n    click E \"Construct the output file path based on parameters.\"\n    click F \"Load the model with or without PEFT adapters.\"\n    click G \"Load the tokenizer for the model.\"\n    click H \"Get all existing instance IDs from the output file to avoid duplicates.\"\n    click I \"Load the dataset, tokenize, filter, and shard it.\"\n    click J \"Generate patches for the dataset using the model.\"\n```",
      "questions": "",
      "checksum": "265036d2d26fc474cea0117d936ead5a"
    }
  ],
  "folders": [
    {
      "folderName": "llamao",
      "folderPath": ".docy/docs/json/inference/llamao",
      "url": "/tree/master/.docy/docs/json/inference/llamao",
      "files": [
        {
          "fileName": "__init__.py",
          "filePath": "inference/llamao/__init__.py",
          "url": "/blob/master/inference/llamao/__init__.py",
          "summary": "```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Configuration]\n    B --> C{Check if Configuration is Valid}\n    C -->|Yes| D[Load Test Data]\n    C -->|No| E[Log Error and Exit]\n    D --> F[Initialize Test Environment]\n    F --> G{Is Environment Ready?}\n    G -->|Yes| H[Run Benchmarks]\n    G -->|No| I[Log Error and Retry Initialization]\n    I --> F\n    H --> J[Collect Benchmark Results]\n    J --> K{Are Results Valid?}\n    K -->|Yes| L[Report Success and Store Results]\n    K -->|No| M[Log Failure and Retry Benchmarks]\n    M --> H\n    L --> N[End]\n```\nThis flowchart represents the logical steps taken by the code in the `swe-bench` project. It starts with initializing the configuration, followed by validating it. If the configuration is invalid, it logs an error and exits. Upon successful validation, it loads test data and initializes the test environment. If the environment is not ready, it logs an error and retries initialization. Once the environment is ready, it runs benchmarks, collects results, and checks their validity. If the results are valid, it reports success and stores the results. If not, it logs a failure and retries the benchmarks.",
          "questions": "",
          "checksum": "74be16979710d4c4e7c6647856088456"
        },
        {
          "fileName": "modeling_flash_llama.py",
          "filePath": "inference/llamao/modeling_flash_llama.py",
          "url": "/blob/master/inference/llamao/modeling_flash_llama.py",
          "summary": "```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Model Components]\n    B --> C{Is it Training?}\n    C -->|Yes| D[Gradient Checkpointing]\n    C -->|No| E[Process Input Embeddings]\n    E --> F[Unpad Input if Required]\n    F --> G[Iterate Over Decoder Layers]\n    G --> H{Is Output Attention?}\n    H -->|Yes| I[Output Attention Weights]\n    H -->|No| J[No Attention Output]\n    I --> K[Post Layer Operations]\n    J --> K\n    K --> L{Is Output Hidden States?}\n    L -->|Yes| M[Output Hidden States]\n    L -->|No| N[No Hidden States Output]\n    M --> O[Apply Final Layer Norm]\n    N --> O\n    O --> P{Is it Sequence Classification?}\n    P -->|Yes| Q[Apply Classification Head]\n    P -->|No| R{Is it Causal LM?}\n    R -->|Yes| S[Apply LM Head]\n    R -->|No| T[Other Task Processing]\n    Q --> U[End]\n    S --> U\n    T --> U\n```\nThis flowchart represents the general workflow of the code provided, focusing on the initialization of model components, handling of input embeddings, processing through decoder layers, and the application of specific heads based on the task (e.g., sequence classification or causal language modeling). It also highlights conditional paths such as gradient checkpointing during training, outputting attention weights, and hidden states based on configuration flags.",
          "questions": "",
          "checksum": "08a7f2980bc080766c0b77557775d1a1"
        }
      ],
      "folders": [],
      "summary": "```mermaid\nflowchart TD\n    subgraph swe_bench [\" \"]\n    start((Start)) --> init_conf[Initialize Configuration]\n    init_conf --> check_conf{Check if Configuration is Valid}\n    check_conf -->|Yes| load_data[Load Test Data]\n    check_conf -->|No| log_error1[Log Error and Exit]\n    load_data --> init_test_env[Initialize Test Environment]\n    init_test_env --> env_ready{Is Environment Ready?}\n    env_ready -->|Yes| run_benchmarks[Run Benchmarks]\n    env_ready -->|No| log_error2[Log Error and Retry Initialization]\n    log_error2 --> init_test_env\n    run_benchmarks --> collect_results[Collect Benchmark Results]\n    collect_results --> results_valid{Are Results Valid?}\n    results_valid -->|Yes| report_success[Report Success and Store Results]\n    results_valid -->|No| log_failure[Log Failure and Retry Benchmarks]\n    log_failure --> run_benchmarks\n    report_success --> end((End))\n    \n    subgraph modeling_flash_llama [\" \"]\n    start_modeling((Start)) --> init_model_comp[Initialize Model Components]\n    init_model_comp --> is_training{Is it Training?}\n    is_training -->|Yes| grad_checkpoint[Gradient Checkpointing]\n    is_training -->|No| proc_input_emb[Process Input Embeddings]\n    proc_input_emb --> unpad_input[Unpad Input if Required]\n    unpad_input --> iterate_dec_layers[Iterate Over Decoder Layers]\n    iterate_dec_layers --> is_out_att{Is Output Attention?}\n    is_out_att -->|Yes| out_att_weights[Output Attention Weights]\n    is_out_att -->|No| no_att_output[No Attention Output]\n    out_att_weights --> post_layer_ops[Post Layer Operations]\n    no_att_output --> post_layer_ops\n    post_layer_ops --> is_out_hidden{Is Output Hidden States?}\n    is_out_hidden -->|Yes| out_hidden_states[Output Hidden States]\n    is_out_hidden -->|No| no_hidden_output[No Hidden States Output]\n    out_hidden_states --> final_layer_norm[Apply Final Layer Norm]\n    no_hidden_output --> final_layer_norm\n    final_layer_norm --> is_seq_class{Is it Sequence Classification?}\n    is_seq_class -->|Yes| apply_class_head[Apply Classification Head]\n    is_seq_class -->|No| is_causal_lm{Is it Causal LM?}\n    is_causal_lm -->|Yes| apply_lm_head[Apply LM Head]\n    is_causal_lm -->|No| other_task_proc[Other Task Processing]\n    apply_class_head --> end_modeling((End))\n    apply_lm_head --> end_modeling\n    other_task_proc --> end_modeling\n    end_modeling --> end\n    end\n    end\n    \n    swe_bench --> modeling_flash_llama\n```\n\nThis mermaid markdown code illustrates the workflow within the `swe-bench` project, specifically focusing on the `.docy/docs/json/inference/llamao` folder. It starts with the initialization and validation of the configuration, followed by the loading of test data and the initialization of the test environment. Upon ensuring the environment is ready, it proceeds to run benchmarks, collect results, and validate them. Parallelly, it details the workflow of `modeling_flash_llama.py`, which includes model component initialization, processing input embeddings, iterating over decoder layers, and applying specific heads based on the task. The diagram also shows how `modeling_flash_llama.py` fits into the larger workflow, indicating that the modeling process is a part of the benchmarking process, potentially contributing to the benchmarks run in the project.",
      "questions": "",
      "checksum": "fd12e72ced1c0022ebee65755ca8193d"
    },
    {
      "folderName": "make_datasets",
      "folderPath": ".docy/docs/json/inference/make_datasets",
      "url": "/tree/master/.docy/docs/json/inference/make_datasets",
      "files": [
        {
          "fileName": "__init__.py",
          "filePath": "inference/make_datasets/__init__.py",
          "url": "/blob/master/inference/make_datasets/__init__.py",
          "summary": "```mermaid\nflowchart TD\n    A[Start] --> B[Initialize Benchmark Environment]\n    B --> C{Check if Benchmark Data Exists}\n    C -->|Yes| D[Load Existing Benchmark Data]\n    C -->|No| E[Generate New Benchmark Data]\n    D --> F[Prepare Benchmark Tests]\n    E --> F\n    F --> G{Is Benchmark Configuration Valid?}\n    G -->|Yes| H[Run Benchmark Tests]\n    G -->|No| I[Log Configuration Error]\n    H --> J[Collect Benchmark Results]\n    J --> K{Are Results Within Expected Range?}\n    K -->|Yes| L[Store Benchmark Results]\n    K -->|No| M[Log Results Out of Range Error]\n    L --> N[Generate Benchmark Report]\n    M --> N\n    N --> O[End]\n```\nThis flowchart represents the steps executed by the code in the `swe-bench` project. It starts with initializing the benchmark environment, followed by checking for existing benchmark data. Depending on whether the data exists, it either loads the existing data or generates new data. After preparing the benchmark tests, it checks if the benchmark configuration is valid. If valid, it runs the benchmark tests; otherwise, it logs a configuration error. Post running the tests, it collects the results and checks if they are within the expected range. If the results are as expected, it stores them and generates a report. If not, it logs an error indicating the results are out of the expected range before generating the report. The process ends after the report generation.",
          "questions": "",
          "checksum": "74be16979710d4c4e7c6647856088456"
        },
        {
          "fileName": "bm25_retrieval.py",
          "filePath": "inference/make_datasets/bm25_retrieval.py",
          "url": "/blob/master/inference/make_datasets/bm25_retrieval.py",
          "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Check if dataset path exists}\n    B -- Yes --> C[Load dataset from disk]\n    B -- No --> D[Load dataset from HuggingFace Datasets]\n    C --> E[Prepare dataset splits]\n    D --> E\n    E --> F{Check for unknown splits}\n    F -- Yes --> G[Error: Unknown splits]\n    F -- No --> H[Get remaining instances]\n    H --> I[Get root directory and name]\n    I --> J[Get index paths for instances]\n    J --> K[Search indexes for instances]\n    K --> L[Get missing IDs]\n    L --> M[Cleanup and finish]\n    M --> N[End]\n```\n\nThis flowchart represents the process outlined in the provided code, focusing on the main steps involved in preparing, processing, and cleaning up after searching indexes for instances in a dataset. The process starts with checking if the dataset path exists, leading to either loading the dataset from disk or from HuggingFace Datasets. After preparing the dataset splits, it checks for unknown splits, which could lead to an error. Then, it proceeds to get the remaining instances that need processing, prepares the root directory, retrieves index paths for instances, and performs searches on these indexes. Finally, it identifies any missing IDs, cleans up resources, and finishes the process.",
          "questions": "",
          "checksum": "f2d5e1f43580981a726649c04dc2a35b"
        },
        {
          "fileName": "create_instance.py",
          "filePath": "inference/make_datasets/create_instance.py",
          "url": "/blob/master/inference/make_datasets/create_instance.py",
          "summary": "```mermaid\ngraph TD\n    A(Start) --> B[Import Libraries and Modules]\n    B --> C[Define Constants and Load External Resources]\n    C --> D[Define Utility Functions]\n    D --> E{Check for File Source}\n    E -->|oracle| F[Ingest Files from Oracle Filenames]\n    E -->|bm25| G[Add Retrieval Results]\n    E -->|all| H[Ingest All Directory Contents]\n    E -->|none| I[Set File Contents to Empty]\n    G --> J[Ingest Files from Retrieval Hits]\n    F --> K[Generate Prompt Based on Style]\n    J --> K\n    H --> K\n    I --> K\n    K --> L[Add Text Inputs to Instances]\n    L --> M[Handle Context Length Limitation]\n    M --> N[Finalize Text Inputs]\n    N --> O[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style O fill:#f9f,stroke:#333,stroke-width:4px\n```",
          "questions": "",
          "checksum": "6fad6e6e19ea4bb02d7f8ed4211f6d0d"
        },
        {
          "fileName": "create_text_dataset.py",
          "filePath": "inference/make_datasets/create_text_dataset.py",
          "url": "/blob/master/inference/make_datasets/create_text_dataset.py",
          "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Parse Arguments]\n    B --> C{Check push_to_hub_user}\n    C -->|None| D[Create output directory if not exists]\n    C -->|Not None| E[Check HUGGING_FACE_HUB_TOKEN and output_dir]\n    B --> F{Check max_context_len}\n    F -->|Not None| G[Assert tokenizer_name is not None]\n    B --> H[Generate output file name based on arguments]\n    H --> I{Check if dataset exists on disk}\n    I -->|Yes| J[Load dataset from disk]\n    I -->|No| K[Load dataset from HuggingFace Datasets]\n    K --> L[Process each split in dataset]\n    L --> M[Add text inputs to instances]\n    M --> N[Extract fields from instances]\n    N --> O[Create Dataset from processed data]\n    O --> P{Check validation_ratio}\n    P -->|>0| Q[Split train dataset into train and validation]\n    P -->|<=0| R[Proceed without splitting]\n    Q --> S[Log number of instances per split]\n    R --> S\n    S --> T{Check push_to_hub_user}\n    T -->|Not None| U[Push dataset to HuggingFace Hub]\n    T -->|None| V[Save dataset to disk]\n    U --> W[End]\n    V --> W\n```\nThis mermaid diagram illustrates the flow of operations in the provided code snippet. It starts with parsing command-line arguments, checks conditions related to `push_to_hub_user` and `max_context_len`, and decides on the dataset loading strategy. It then processes each dataset split by adding text inputs, extracting necessary fields, and creating a `Dataset` object. Depending on the `validation_ratio`, it may split the training dataset. Finally, it either pushes the dataset to the HuggingFace Hub or saves it to disk, based on the `push_to_hub_user` argument.",
          "questions": "",
          "checksum": "1a24470e01024da31b02a08e1b4b66b3"
        },
        {
          "fileName": "tokenize_dataset.py",
          "filePath": "inference/make_datasets/tokenize_dataset.py",
          "url": "/blob/master/inference/make_datasets/tokenize_dataset.py",
          "summary": "```mermaid\nflowchart TD\n    A[Start] --> B{Check if tokenizer name provided}\n    B -->|Yes| C[Initialize tokenizer and function]\n    B -->|No| Z[End without tokenizer initialization]\n    C --> D{Check if dataset path exists}\n    D -->|Yes| E[Load dataset from disk]\n    D -->|No| F[Load dataset using name or path]\n    E & F --> G[Filter out superlong instances]\n    G --> H{Is split test?}\n    H -->|No| I[Tokenize non-test split]\n    H -->|Yes| J[Skip tokenizing test split]\n    I --> K{Is num_proc > 0?}\n    K -->|Yes| L[Tokenize with multiprocessing]\n    K -->|No| M[Tokenize without multiprocessing]\n    L & M --> N[Add tokenized columns to dataset]\n    J --> O{Is num_proc > 0?}\n    O -->|Yes| P[Tokenize test split with multiprocessing]\n    O -->|No| Q[Tokenize test split without multiprocessing]\n    P & Q --> R[Add tokenized columns to test dataset]\n    N & R --> S{Is push to hub user specified?}\n    S -->|Yes| T[Push dataset to Hugging Face Hub]\n    S -->|No| U[Save dataset to disk]\n    T & U --> Z[End]\n```",
          "questions": "",
          "checksum": "fed2d027705839539fd984bd10274e6d"
        },
        {
          "fileName": "utils.py",
          "filePath": "inference/make_datasets/utils.py",
          "url": "/blob/master/inference/make_datasets/utils.py",
          "summary": "```mermaid\ngraph TD\n    A[Start] --> B[Extract Diff]\n    B --> C{Is Test?}\n    C -->|Yes| D[Skip File]\n    C -->|No| E[Detect Encoding]\n    E --> F[Read File Content]\n    F --> G[Ingest Directory Contents]\n    G --> H[Ingest File Directory Contents]\n    H --> I[Repair Patch]\n    I --> J[Extract Minimal Patch]\n    J --> K[End]\n\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style K fill:#f9f,stroke:#333,stroke-width:4px\n```\nThis flowchart represents the sequence of operations performed by the code in the provided context. It starts with extracting diffs from a given input, checks if the file is a test file, detects the file encoding, reads the file content, ingests directory contents, ingests file directory contents, repairs patches, and finally extracts a minimal patch from the given input. The process begins at \"Start\" and ends at \"End\", with decision points and operations represented as nodes in the flowchart.",
          "questions": "",
          "checksum": "4522388584c3bb89a5872a909821cea4"
        }
      ],
      "folders": [],
      "summary": "```mermaid\nflowchart TB\n    subgraph swe_bench [\" \"]\n    start((Start))\n    init[(\"/__init__.py\\nInitialize Benchmark Environment\")]\n    bm25[(\"/bm25_retrieval.py\\nBM25 Dataset Retrieval\")]\n    create_inst[(\"/create_instance.py\\nCreate Instance\")]\n    create_text_ds[(\"/create_text_dataset.py\\nCreate Text Dataset\")]\n    tokenize_ds[(\"/tokenize_dataset.py\\nTokenize Dataset\")]\n    utils[(\"/utils.py\\nUtility Functions\")]\n    end((End))\n\n    start --> init\n    init --> bm25\n    init --> create_inst\n    init --> create_text_ds\n    init --> tokenize_ds\n    init --> utils\n    bm25 --> create_text_ds\n    create_inst --> create_text_ds\n    tokenize_ds --> create_text_ds\n    utils --> create_inst\n    utils --> bm25\n    create_text_ds --> end\n    end\n    end\n```\n\nThis diagram illustrates the interconnected steps and processes within the `make_datasets` folder of the project. It starts with initializing the benchmark environment, followed by various processes such as BM25 dataset retrieval, instance creation, text dataset creation, dataset tokenization, and utility functions. Each script contributes to the preparation and processing of datasets, ultimately leading to the creation of a text dataset that is ready for further actions, such as benchmarking or analysis. Utility functions support various steps in the process, indicating a cohesive workflow aimed at dataset preparation and processing within the project.",
      "questions": "",
      "checksum": "0f34dfbe0d073e32b17eef7664627f7d"
    }
  ],
  "summary": "```mermaid\nflowchart TB\n    subgraph inference_process [\"Inference Process\"]\n        start((Start)) --> init_env[Initialize swe-bench Environment]\n        init_env --> data_check{Check if Data Exists}\n        data_check -->|Yes| load_data[Load Existing Data]\n        data_check -->|No| create_data[Create New Data]\n        load_data --> process_data[Process Data]\n        create_data --> process_data\n        process_data --> processing_complete{Is Processing Complete?}\n        processing_complete -->|Yes| save_data[Save Processed Data]\n        processing_complete -->|No| process_data\n        save_data --> generate_reports[Generate Reports]\n        generate_reports --> end((End))\n    end\n\n    subgraph model_processing [\"Model Processing\"]\n        model_start((Start)) --> init_model[Initialize Model Components]\n        init_model --> component_type{Component Type}\n        component_type -->|Embed Tokens| embedding_layer[Embedding Layer]\n        component_type -->|Model Layers| processing_layers[Processing Layers]\n        component_type -->|Norm| normalization_layer[Normalization Layer]\n        component_type -->|LM Head| language_model_head[Language Model Head]\n        embedding_layer --> token_embedding_proc[Token Embedding Processing]\n        processing_layers --> layer_wise_proc[Layer-wise Processing]\n        normalization_layer --> normalization_proc[Normalization Processing]\n        language_model_head --> language_model_output[Language Model Output]\n        layer_wise_proc --> layer_activation{Layer Activation}\n        layer_activation -->|0| no_activation[No Activation]\n        layer_activation -->|1| activation_lvl1[Activation Level 1]\n        layer_activation -->|2| activation_lvl2[Activation Level 2]\n        layer_activation -->|3| activation_lvl3[Activation Level 3]\n        no_activation --> output_next_layer[Output to Next Layer/Process]\n        activation_lvl1 --> output_next_layer\n        activation_lvl2 --> output_next_layer\n        activation_lvl3 --> output_next_layer\n        output_next_layer --> next_component{Next Component}\n        next_component -->|Model Layers| processing_layers\n        next_component -->|Norm| normalization_layer\n        next_component -->|LM Head| language_model_head\n        next_component -->|End| model_end((End of Model Processing))\n    end\n\n    subgraph environment_setup [\"Environment Setup\"]\n        env_start((Start)) --> define_channels[Define Channels]\n        define_channels --> define_dependencies[Define Dependencies]\n        define_dependencies --> base_env_deps[Base Environment Dependencies]\n        base_env_deps --> python_core_libs[Python and Core Libraries]\n        python_core_libs --> dev_tools[Development Tools]\n        dev_tools --> define_pip_deps[Define pip Dependencies]\n        define_pip_deps --> core_python_libs[Core Python Libraries]\n        core_python_libs --> data_ai_libs[Data Processing and AI Libraries]\n        data_ai_libs --> web_net_libs[Web and Networking Libraries]\n        web_net_libs --> dev_debug_tools[Development and Debugging Tools]\n        dev_debug_tools --> env_end((End))\n    end\n\n    subgraph api_inference [\"API Inference\"]\n        api_start((Start)) --> check_model_prefix{Check Model Prefix}\n        check_model_prefix -->|claude| anthropic_inference[Anthropic Inference]\n        check_model_prefix -->|gpt| openai_inference[OpenAI Inference]\n        anthropic_inference --> load_dataset[Load Dataset]\n        openai_inference --> load_dataset\n        load_dataset --> filter_dataset{Filter Dataset}\n        filter_dataset -->|Existing IDs| filter_existing_ids[Filter out existing IDs]\n        filter_dataset -->|Shard Info| select_shard[Select Shard]\n        filter_existing_ids --> select_shard\n        select_shard --> inference_loop[Inference Loop]\n        inference_loop --> check_max_cost{Check Max Cost}\n        check_max_cost -->|Reached| exit[Exit]\n        check_max_cost -->|Not Reached| inference_loop\n        exit --> api_end((End))\n    end\n\n    subgraph live_inference [\"Live Inference\"]\n        live_start((Start)) --> parse_issue_url{Parse Issue URL}\n        parse_issue_url -->|Valid| get_problem_statement[Get Problem Statement]\n        parse_issue_url -->|Invalid| error_invalid_url[Error: Invalid URL]\n        get_problem_statement --> clone_repo[Clone Repository]\n        clone_repo --> build_bm25_index[Build BM25 Retrieval Index]\n        build_bm25_index --> search_index[Search in Index]\n        search_index --> include_readmes{Include READMEs?}\n        include_readmes -->|Yes| get_readme_files[Get README Files]\n        include_readmes -->|No| skip_readmes[Skip READMEs]\n        get_readme_files --> ingest_files[Ingest Files]\n        skip_readmes --> ingest_files\n        ingest_files --> generate_prompt[Generate Prompt]\n        generate_prompt --> call_model[Call Model]\n        call_model --> extract_diff[Extract Diff from Response]\n        extract_diff --> extract_minimal_patch[Extract Minimal Patch]\n        extract_minimal_patch --> save_output[Save Output]\n        save_output --> live_end((End))\n    end\n\n    subgraph dataset_preparation [\"Dataset Preparation\"]\n        ds_start((Start)) --> init_bench_env[Initialize Benchmark Environment]\n        init_bench_env --> bm25_retrieval[BM25 Dataset Retrieval]\n        init_bench_env --> create_instance[Create Instance]\n        init_bench_env --> create_text_dataset[Create Text Dataset]\n        init_bench_env --> tokenize_dataset[Tokenize Dataset]\n        init_bench_env --> util_funcs[Utility Functions]\n        bm25_retrieval --> create_text_dataset\n        create_instance --> create_text_dataset\n        tokenize_dataset --> create_text_dataset\n        util_funcs --> create_instance\n        util_funcs --> bm25_retrieval\n        create_text_dataset --> ds_end((End))\n    end\n\n    inference_process --> model_processing --> environment_setup --> api_inference --> live_inference --> dataset_preparation\n```",
  "questions": "",
  "checksum": "12bd289d952793d0dff8cd529f2f3274"
}